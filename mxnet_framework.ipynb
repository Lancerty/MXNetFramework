{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgotq2V6M_zB"
      },
      "source": [
        "**MXNet Deep Learning Framework**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijcviKPsNar-"
      },
      "source": [
        "1. Install and Import the necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw2sk1C0M9Vn",
        "outputId": "b551b0bc-744d-4741-bcb7-f23bb3fbf98f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.27.1)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "import mxnet as mx\n",
        "from mxnet import gluon, autograd, nd\n",
        "from graphviz import Digraph\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmBDXvsmN4xf"
      },
      "source": [
        "2. Define the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XiaE0184N8Gh"
      },
      "outputs": [],
      "source": [
        "# Define the neural network model\n",
        "neurons = 5\n",
        "net = gluon.nn.Sequential()\n",
        "with net.name_scope():\n",
        "    net.add(gluon.nn.Dense(neurons, activation='relu'))\n",
        "    net.add(gluon.nn.Dense(1))\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Model Data Initialization\n",
        "\n"
      ],
      "metadata": {
        "id": "4p9tCLz85GuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data set\n",
        "iris = load_iris()\n",
        "data = nd.array(iris.data)\n",
        "\n",
        "# Normalize the data\n",
        "data_mean = nd.mean(data, axis=0)\n",
        "data -= data_mean\n",
        "data /= nd.sqrt(nd.mean(nd.square(data)))\n",
        "\n",
        "labels = nd.array(iris.target)\n",
        "\n",
        "# Define the neural network model\n",
        "net = gluon.nn.Sequential()\n",
        "with net.name_scope():\n",
        "    net.add(gluon.nn.Dense(5, activation='relu'))\n",
        "    net.add(gluon.nn.Dense(1))\n",
        "    \n",
        "# Split data into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.8)\n",
        "\n",
        "# Initialize the model parameters\n",
        "net.collect_params().initialize(mx.init.Normal(sigma=0.1))\n",
        "\n",
        "# Define the loss function\n",
        "loss = gluon.loss.L2Loss()\n",
        "\n",
        "# Define the optimizer\n",
        "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFl1s4cd3_BX",
        "outputId": "313748fb-8f5b-43f8-8001-76128af2d9c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, training loss: 0.694424\n",
            "Epoch 1, training loss: 0.622301\n",
            "Epoch 2, training loss: 0.565619\n",
            "Epoch 3, training loss: 0.520862\n",
            "Epoch 4, training loss: 0.485622\n",
            "Epoch 5, training loss: 0.457665\n",
            "Epoch 6, training loss: 0.434943\n",
            "Epoch 7, training loss: 0.416047\n",
            "Epoch 8, training loss: 0.400460\n",
            "Epoch 9, training loss: 0.387272\n",
            "Epoch 10, training loss: 0.375872\n",
            "Epoch 11, training loss: 0.365791\n",
            "Epoch 12, training loss: 0.356680\n",
            "Epoch 13, training loss: 0.348235\n",
            "Epoch 14, training loss: 0.340204\n",
            "Epoch 15, training loss: 0.332378\n",
            "Epoch 16, training loss: 0.324517\n",
            "Epoch 17, training loss: 0.316460\n",
            "Epoch 18, training loss: 0.308094\n",
            "Epoch 19, training loss: 0.299236\n",
            "Epoch 20, training loss: 0.289740\n",
            "Epoch 21, training loss: 0.279526\n",
            "Epoch 22, training loss: 0.268585\n",
            "Epoch 23, training loss: 0.256902\n",
            "Epoch 24, training loss: 0.244489\n",
            "Epoch 25, training loss: 0.231418\n",
            "Epoch 26, training loss: 0.217752\n",
            "Epoch 27, training loss: 0.203599\n",
            "Epoch 28, training loss: 0.189150\n",
            "Epoch 29, training loss: 0.174588\n",
            "Epoch 30, training loss: 0.160090\n",
            "Epoch 31, training loss: 0.145874\n",
            "Epoch 32, training loss: 0.132169\n",
            "Epoch 33, training loss: 0.119119\n",
            "Epoch 34, training loss: 0.106866\n",
            "Epoch 35, training loss: 0.095523\n",
            "Epoch 36, training loss: 0.085169\n",
            "Epoch 37, training loss: 0.075851\n",
            "Epoch 38, training loss: 0.067585\n",
            "Epoch 39, training loss: 0.060356\n",
            "Epoch 40, training loss: 0.054127\n",
            "Epoch 41, training loss: 0.048837\n",
            "Epoch 42, training loss: 0.044408\n",
            "Epoch 43, training loss: 0.040748\n",
            "Epoch 44, training loss: 0.037762\n",
            "Epoch 45, training loss: 0.035354\n",
            "Epoch 46, training loss: 0.033431\n",
            "Epoch 47, training loss: 0.031908\n",
            "Epoch 48, training loss: 0.030707\n",
            "Epoch 49, training loss: 0.029763\n",
            "Epoch 50, training loss: 0.029019\n",
            "Epoch 51, training loss: 0.028430\n",
            "Epoch 52, training loss: 0.027960\n",
            "Epoch 53, training loss: 0.027580\n",
            "Epoch 54, training loss: 0.027268\n",
            "Epoch 55, training loss: 0.027006\n",
            "Epoch 56, training loss: 0.026782\n",
            "Epoch 57, training loss: 0.026587\n",
            "Epoch 58, training loss: 0.026412\n",
            "Epoch 59, training loss: 0.026254\n",
            "Epoch 60, training loss: 0.026108\n",
            "Epoch 61, training loss: 0.025971\n",
            "Epoch 62, training loss: 0.025842\n",
            "Epoch 63, training loss: 0.025719\n",
            "Epoch 64, training loss: 0.025601\n",
            "Epoch 65, training loss: 0.025486\n",
            "Epoch 66, training loss: 0.025375\n",
            "Epoch 67, training loss: 0.025268\n",
            "Epoch 68, training loss: 0.025158\n",
            "Epoch 69, training loss: 0.024963\n",
            "Epoch 70, training loss: 0.024775\n",
            "Epoch 71, training loss: 0.024593\n",
            "Epoch 72, training loss: 0.024417\n",
            "Epoch 73, training loss: 0.024247\n",
            "Epoch 74, training loss: 0.024082\n",
            "Epoch 75, training loss: 0.023923\n",
            "Epoch 76, training loss: 0.023769\n",
            "Epoch 77, training loss: 0.023620\n",
            "Epoch 78, training loss: 0.023475\n",
            "Epoch 79, training loss: 0.023336\n",
            "Epoch 80, training loss: 0.023200\n",
            "Epoch 81, training loss: 0.023069\n",
            "Epoch 82, training loss: 0.022942\n",
            "Epoch 83, training loss: 0.022819\n",
            "Epoch 84, training loss: 0.022699\n",
            "Epoch 85, training loss: 0.022583\n",
            "Epoch 86, training loss: 0.022470\n",
            "Epoch 87, training loss: 0.022360\n",
            "Epoch 88, training loss: 0.022254\n",
            "Epoch 89, training loss: 0.022151\n",
            "Epoch 90, training loss: 0.022051\n",
            "Epoch 91, training loss: 0.021953\n",
            "Epoch 92, training loss: 0.021858\n",
            "Epoch 93, training loss: 0.021766\n",
            "Epoch 94, training loss: 0.021676\n",
            "Epoch 95, training loss: 0.021589\n",
            "Epoch 96, training loss: 0.021504\n",
            "Epoch 97, training loss: 0.021421\n",
            "Epoch 98, training loss: 0.021340\n",
            "Epoch 99, training loss: 0.021262\n",
            "Epoch 100, training loss: 0.021185\n",
            "Epoch 101, training loss: 0.021110\n",
            "Epoch 102, training loss: 0.021038\n",
            "Epoch 103, training loss: 0.020966\n",
            "Epoch 104, training loss: 0.020897\n",
            "Epoch 105, training loss: 0.020829\n",
            "Epoch 106, training loss: 0.020763\n",
            "Epoch 107, training loss: 0.020699\n",
            "Epoch 108, training loss: 0.020635\n",
            "Epoch 109, training loss: 0.020573\n",
            "Epoch 110, training loss: 0.020512\n",
            "Epoch 111, training loss: 0.020452\n",
            "Epoch 112, training loss: 0.020393\n",
            "Epoch 113, training loss: 0.020335\n",
            "Epoch 114, training loss: 0.020279\n",
            "Epoch 115, training loss: 0.020224\n",
            "Epoch 116, training loss: 0.020170\n",
            "Epoch 117, training loss: 0.020117\n",
            "Epoch 118, training loss: 0.020065\n",
            "Epoch 119, training loss: 0.020013\n",
            "Epoch 120, training loss: 0.019963\n",
            "Epoch 121, training loss: 0.019914\n",
            "Epoch 122, training loss: 0.019866\n",
            "Epoch 123, training loss: 0.019818\n",
            "Epoch 124, training loss: 0.019772\n",
            "Epoch 125, training loss: 0.019726\n",
            "Epoch 126, training loss: 0.019681\n",
            "Epoch 127, training loss: 0.019637\n",
            "Epoch 128, training loss: 0.019593\n",
            "Epoch 129, training loss: 0.019550\n",
            "Epoch 130, training loss: 0.019508\n",
            "Epoch 131, training loss: 0.019467\n",
            "Epoch 132, training loss: 0.019426\n",
            "Epoch 133, training loss: 0.019385\n",
            "Epoch 134, training loss: 0.019346\n",
            "Epoch 135, training loss: 0.019307\n",
            "Epoch 136, training loss: 0.019268\n",
            "Epoch 137, training loss: 0.019230\n",
            "Epoch 138, training loss: 0.019193\n",
            "Epoch 139, training loss: 0.019156\n",
            "Epoch 140, training loss: 0.019119\n",
            "Epoch 141, training loss: 0.019083\n",
            "Epoch 142, training loss: 0.019048\n",
            "Epoch 143, training loss: 0.019013\n",
            "Epoch 144, training loss: 0.018978\n",
            "Epoch 145, training loss: 0.018944\n",
            "Epoch 146, training loss: 0.018910\n",
            "Epoch 147, training loss: 0.018876\n",
            "Epoch 148, training loss: 0.018843\n",
            "Epoch 149, training loss: 0.018811\n",
            "Epoch 150, training loss: 0.018778\n",
            "Epoch 151, training loss: 0.018747\n",
            "Epoch 152, training loss: 0.018715\n",
            "Epoch 153, training loss: 0.018684\n",
            "Epoch 154, training loss: 0.018653\n",
            "Epoch 155, training loss: 0.018622\n",
            "Epoch 156, training loss: 0.018592\n",
            "Epoch 157, training loss: 0.018562\n",
            "Epoch 158, training loss: 0.018532\n",
            "Epoch 159, training loss: 0.018503\n",
            "Epoch 160, training loss: 0.018474\n",
            "Epoch 161, training loss: 0.018445\n",
            "Epoch 162, training loss: 0.018416\n",
            "Epoch 163, training loss: 0.018388\n",
            "Epoch 164, training loss: 0.018360\n",
            "Epoch 165, training loss: 0.018332\n",
            "Epoch 166, training loss: 0.018304\n",
            "Epoch 167, training loss: 0.018277\n",
            "Epoch 168, training loss: 0.018250\n",
            "Epoch 169, training loss: 0.018223\n",
            "Epoch 170, training loss: 0.018196\n",
            "Epoch 171, training loss: 0.018170\n",
            "Epoch 172, training loss: 0.018143\n",
            "Epoch 173, training loss: 0.018117\n",
            "Epoch 174, training loss: 0.018091\n",
            "Epoch 175, training loss: 0.018066\n",
            "Epoch 176, training loss: 0.018040\n",
            "Epoch 177, training loss: 0.018015\n",
            "Epoch 178, training loss: 0.017990\n",
            "Epoch 179, training loss: 0.017965\n",
            "Epoch 180, training loss: 0.017940\n",
            "Epoch 181, training loss: 0.017915\n",
            "Epoch 182, training loss: 0.017891\n",
            "Epoch 183, training loss: 0.017867\n",
            "Epoch 184, training loss: 0.017843\n",
            "Epoch 185, training loss: 0.017819\n",
            "Epoch 186, training loss: 0.017795\n",
            "Epoch 187, training loss: 0.017771\n",
            "Epoch 188, training loss: 0.017748\n",
            "Epoch 189, training loss: 0.017725\n",
            "Epoch 190, training loss: 0.017702\n",
            "Epoch 191, training loss: 0.017679\n",
            "Epoch 192, training loss: 0.017656\n",
            "Epoch 193, training loss: 0.017633\n",
            "Epoch 194, training loss: 0.017610\n",
            "Epoch 195, training loss: 0.017588\n",
            "Epoch 196, training loss: 0.017566\n",
            "Epoch 197, training loss: 0.017544\n",
            "Epoch 198, training loss: 0.017522\n",
            "Epoch 199, training loss: 0.017500\n",
            "Epoch 200, training loss: 0.017478\n",
            "Epoch 201, training loss: 0.017456\n",
            "Epoch 202, training loss: 0.017435\n",
            "Epoch 203, training loss: 0.017413\n",
            "Epoch 204, training loss: 0.017392\n",
            "Epoch 205, training loss: 0.017371\n",
            "Epoch 206, training loss: 0.017350\n",
            "Epoch 207, training loss: 0.017329\n",
            "Epoch 208, training loss: 0.017308\n",
            "Epoch 209, training loss: 0.017287\n",
            "Epoch 210, training loss: 0.017267\n",
            "Epoch 211, training loss: 0.017246\n",
            "Epoch 212, training loss: 0.017226\n",
            "Epoch 213, training loss: 0.017206\n",
            "Epoch 214, training loss: 0.017186\n",
            "Epoch 215, training loss: 0.017166\n",
            "Epoch 216, training loss: 0.017146\n",
            "Epoch 217, training loss: 0.017126\n",
            "Epoch 218, training loss: 0.017106\n",
            "Epoch 219, training loss: 0.017086\n",
            "Epoch 220, training loss: 0.017067\n",
            "Epoch 221, training loss: 0.017047\n",
            "Epoch 222, training loss: 0.017028\n",
            "Epoch 223, training loss: 0.017009\n",
            "Epoch 224, training loss: 0.016990\n",
            "Epoch 225, training loss: 0.016971\n",
            "Epoch 226, training loss: 0.016952\n",
            "Epoch 227, training loss: 0.016933\n",
            "Epoch 228, training loss: 0.016914\n",
            "Epoch 229, training loss: 0.016896\n",
            "Epoch 230, training loss: 0.016877\n",
            "Epoch 231, training loss: 0.016859\n",
            "Epoch 232, training loss: 0.016840\n",
            "Epoch 233, training loss: 0.016822\n",
            "Epoch 234, training loss: 0.016804\n",
            "Epoch 235, training loss: 0.016786\n",
            "Epoch 236, training loss: 0.016768\n",
            "Epoch 237, training loss: 0.016750\n",
            "Epoch 238, training loss: 0.016734\n",
            "Epoch 239, training loss: 0.016718\n",
            "Epoch 240, training loss: 0.016702\n",
            "Epoch 241, training loss: 0.016686\n",
            "Epoch 242, training loss: 0.016671\n",
            "Epoch 243, training loss: 0.016655\n",
            "Epoch 244, training loss: 0.016640\n",
            "Epoch 245, training loss: 0.016625\n",
            "Epoch 246, training loss: 0.016609\n",
            "Epoch 247, training loss: 0.016594\n",
            "Epoch 248, training loss: 0.016579\n",
            "Epoch 249, training loss: 0.016564\n",
            "Epoch 250, training loss: 0.016549\n",
            "Epoch 251, training loss: 0.016534\n",
            "Epoch 252, training loss: 0.016519\n",
            "Epoch 253, training loss: 0.016505\n",
            "Epoch 254, training loss: 0.016490\n",
            "Epoch 255, training loss: 0.016475\n",
            "Epoch 256, training loss: 0.016461\n",
            "Epoch 257, training loss: 0.016447\n",
            "Epoch 258, training loss: 0.016432\n",
            "Epoch 259, training loss: 0.016418\n",
            "Epoch 260, training loss: 0.016404\n",
            "Epoch 261, training loss: 0.016389\n",
            "Epoch 262, training loss: 0.016375\n",
            "Epoch 263, training loss: 0.016361\n",
            "Epoch 264, training loss: 0.016347\n",
            "Epoch 265, training loss: 0.016333\n",
            "Epoch 266, training loss: 0.016319\n",
            "Epoch 267, training loss: 0.016306\n",
            "Epoch 268, training loss: 0.016292\n",
            "Epoch 269, training loss: 0.016278\n",
            "Epoch 270, training loss: 0.016265\n",
            "Epoch 271, training loss: 0.016251\n",
            "Epoch 272, training loss: 0.016238\n",
            "Epoch 273, training loss: 0.016224\n",
            "Epoch 274, training loss: 0.016211\n",
            "Epoch 275, training loss: 0.016198\n",
            "Epoch 276, training loss: 0.016184\n",
            "Epoch 277, training loss: 0.016171\n",
            "Epoch 278, training loss: 0.016158\n",
            "Epoch 279, training loss: 0.016145\n",
            "Epoch 280, training loss: 0.016132\n",
            "Epoch 281, training loss: 0.016119\n",
            "Epoch 282, training loss: 0.016106\n",
            "Epoch 283, training loss: 0.016093\n",
            "Epoch 284, training loss: 0.016081\n",
            "Epoch 285, training loss: 0.016068\n",
            "Epoch 286, training loss: 0.016055\n",
            "Epoch 287, training loss: 0.016043\n",
            "Epoch 288, training loss: 0.016030\n",
            "Epoch 289, training loss: 0.016018\n",
            "Epoch 290, training loss: 0.016005\n",
            "Epoch 291, training loss: 0.015993\n",
            "Epoch 292, training loss: 0.015981\n",
            "Epoch 293, training loss: 0.015968\n",
            "Epoch 294, training loss: 0.015956\n",
            "Epoch 295, training loss: 0.015944\n",
            "Epoch 296, training loss: 0.015932\n",
            "Epoch 297, training loss: 0.015920\n",
            "Epoch 298, training loss: 0.015908\n",
            "Epoch 299, training loss: 0.015896\n",
            "Epoch 300, training loss: 0.015885\n",
            "Epoch 301, training loss: 0.015873\n",
            "Epoch 302, training loss: 0.015861\n",
            "Epoch 303, training loss: 0.015849\n",
            "Epoch 304, training loss: 0.015838\n",
            "Epoch 305, training loss: 0.015826\n",
            "Epoch 306, training loss: 0.015815\n",
            "Epoch 307, training loss: 0.015803\n",
            "Epoch 308, training loss: 0.015792\n",
            "Epoch 309, training loss: 0.015781\n",
            "Epoch 310, training loss: 0.015769\n",
            "Epoch 311, training loss: 0.015758\n",
            "Epoch 312, training loss: 0.015747\n",
            "Epoch 313, training loss: 0.015736\n",
            "Epoch 314, training loss: 0.015725\n",
            "Epoch 315, training loss: 0.015714\n",
            "Epoch 316, training loss: 0.015703\n",
            "Epoch 317, training loss: 0.015692\n",
            "Epoch 318, training loss: 0.015681\n",
            "Epoch 319, training loss: 0.015670\n",
            "Epoch 320, training loss: 0.015659\n",
            "Epoch 321, training loss: 0.015649\n",
            "Epoch 322, training loss: 0.015638\n",
            "Epoch 323, training loss: 0.015628\n",
            "Epoch 324, training loss: 0.015617\n",
            "Epoch 325, training loss: 0.015606\n",
            "Epoch 326, training loss: 0.015596\n",
            "Epoch 327, training loss: 0.015586\n",
            "Epoch 328, training loss: 0.015575\n",
            "Epoch 329, training loss: 0.015565\n",
            "Epoch 330, training loss: 0.015555\n",
            "Epoch 331, training loss: 0.015545\n",
            "Epoch 332, training loss: 0.015534\n",
            "Epoch 333, training loss: 0.015524\n",
            "Epoch 334, training loss: 0.015514\n",
            "Epoch 335, training loss: 0.015504\n",
            "Epoch 336, training loss: 0.015494\n",
            "Epoch 337, training loss: 0.015484\n",
            "Epoch 338, training loss: 0.015475\n",
            "Epoch 339, training loss: 0.015465\n",
            "Epoch 340, training loss: 0.015455\n",
            "Epoch 341, training loss: 0.015445\n",
            "Epoch 342, training loss: 0.015436\n",
            "Epoch 343, training loss: 0.015426\n",
            "Epoch 344, training loss: 0.015416\n",
            "Epoch 345, training loss: 0.015407\n",
            "Epoch 346, training loss: 0.015397\n",
            "Epoch 347, training loss: 0.015388\n",
            "Epoch 348, training loss: 0.015379\n",
            "Epoch 349, training loss: 0.015369\n",
            "Epoch 350, training loss: 0.015360\n",
            "Epoch 351, training loss: 0.015351\n",
            "Epoch 352, training loss: 0.015342\n",
            "Epoch 353, training loss: 0.015332\n",
            "Epoch 354, training loss: 0.015323\n",
            "Epoch 355, training loss: 0.015314\n",
            "Epoch 356, training loss: 0.015305\n",
            "Epoch 357, training loss: 0.015296\n",
            "Epoch 358, training loss: 0.015287\n",
            "Epoch 359, training loss: 0.015278\n",
            "Epoch 360, training loss: 0.015270\n",
            "Epoch 361, training loss: 0.015261\n",
            "Epoch 362, training loss: 0.015252\n",
            "Epoch 363, training loss: 0.015243\n",
            "Epoch 364, training loss: 0.015235\n",
            "Epoch 365, training loss: 0.015226\n",
            "Epoch 366, training loss: 0.015217\n",
            "Epoch 367, training loss: 0.015209\n",
            "Epoch 368, training loss: 0.015200\n",
            "Epoch 369, training loss: 0.015192\n",
            "Epoch 370, training loss: 0.015183\n",
            "Epoch 371, training loss: 0.015175\n",
            "Epoch 372, training loss: 0.015167\n",
            "Epoch 373, training loss: 0.015158\n",
            "Epoch 374, training loss: 0.015150\n",
            "Epoch 375, training loss: 0.015142\n",
            "Epoch 376, training loss: 0.015134\n",
            "Epoch 377, training loss: 0.015126\n",
            "Epoch 378, training loss: 0.015118\n",
            "Epoch 379, training loss: 0.015109\n",
            "Epoch 380, training loss: 0.015101\n",
            "Epoch 381, training loss: 0.015093\n",
            "Epoch 382, training loss: 0.015086\n",
            "Epoch 383, training loss: 0.015078\n",
            "Epoch 384, training loss: 0.015070\n",
            "Epoch 385, training loss: 0.015062\n",
            "Epoch 386, training loss: 0.015054\n",
            "Epoch 387, training loss: 0.015047\n",
            "Epoch 388, training loss: 0.015039\n",
            "Epoch 389, training loss: 0.015031\n",
            "Epoch 390, training loss: 0.015024\n",
            "Epoch 391, training loss: 0.015016\n",
            "Epoch 392, training loss: 0.015008\n",
            "Epoch 393, training loss: 0.015001\n",
            "Epoch 394, training loss: 0.014993\n",
            "Epoch 395, training loss: 0.014986\n",
            "Epoch 396, training loss: 0.014979\n",
            "Epoch 397, training loss: 0.014971\n",
            "Epoch 398, training loss: 0.014964\n",
            "Epoch 399, training loss: 0.014957\n",
            "Epoch 400, training loss: 0.014949\n",
            "Epoch 401, training loss: 0.014942\n",
            "Epoch 402, training loss: 0.014935\n",
            "Epoch 403, training loss: 0.014928\n",
            "Epoch 404, training loss: 0.014921\n",
            "Epoch 405, training loss: 0.014914\n",
            "Epoch 406, training loss: 0.014907\n",
            "Epoch 407, training loss: 0.014900\n",
            "Epoch 408, training loss: 0.014893\n",
            "Epoch 409, training loss: 0.014886\n",
            "Epoch 410, training loss: 0.014879\n",
            "Epoch 411, training loss: 0.014872\n",
            "Epoch 412, training loss: 0.014865\n",
            "Epoch 413, training loss: 0.014858\n",
            "Epoch 414, training loss: 0.014852\n",
            "Epoch 415, training loss: 0.014845\n",
            "Epoch 416, training loss: 0.014838\n",
            "Epoch 417, training loss: 0.014831\n",
            "Epoch 418, training loss: 0.014825\n",
            "Epoch 419, training loss: 0.014818\n",
            "Epoch 420, training loss: 0.014812\n",
            "Epoch 421, training loss: 0.014805\n",
            "Epoch 422, training loss: 0.014799\n",
            "Epoch 423, training loss: 0.014792\n",
            "Epoch 424, training loss: 0.014786\n",
            "Epoch 425, training loss: 0.014779\n",
            "Epoch 426, training loss: 0.014773\n",
            "Epoch 427, training loss: 0.014767\n",
            "Epoch 428, training loss: 0.014760\n",
            "Epoch 429, training loss: 0.014754\n",
            "Epoch 430, training loss: 0.014748\n",
            "Epoch 431, training loss: 0.014742\n",
            "Epoch 432, training loss: 0.014736\n",
            "Epoch 433, training loss: 0.014729\n",
            "Epoch 434, training loss: 0.014723\n",
            "Epoch 435, training loss: 0.014717\n",
            "Epoch 436, training loss: 0.014711\n",
            "Epoch 437, training loss: 0.014705\n",
            "Epoch 438, training loss: 0.014699\n",
            "Epoch 439, training loss: 0.014693\n",
            "Epoch 440, training loss: 0.014687\n",
            "Epoch 441, training loss: 0.014681\n",
            "Epoch 442, training loss: 0.014675\n",
            "Epoch 443, training loss: 0.014670\n",
            "Epoch 444, training loss: 0.014664\n",
            "Epoch 445, training loss: 0.014658\n",
            "Epoch 446, training loss: 0.014652\n",
            "Epoch 447, training loss: 0.014647\n",
            "Epoch 448, training loss: 0.014641\n",
            "Epoch 449, training loss: 0.014635\n",
            "Epoch 450, training loss: 0.014630\n",
            "Epoch 451, training loss: 0.014624\n",
            "Epoch 452, training loss: 0.014618\n",
            "Epoch 453, training loss: 0.014613\n",
            "Epoch 454, training loss: 0.014607\n",
            "Epoch 455, training loss: 0.014602\n",
            "Epoch 456, training loss: 0.014596\n",
            "Epoch 457, training loss: 0.014591\n",
            "Epoch 458, training loss: 0.014585\n",
            "Epoch 459, training loss: 0.014580\n",
            "Epoch 460, training loss: 0.014575\n",
            "Epoch 461, training loss: 0.014569\n",
            "Epoch 462, training loss: 0.014564\n",
            "Epoch 463, training loss: 0.014559\n",
            "Epoch 464, training loss: 0.014554\n",
            "Epoch 465, training loss: 0.014548\n",
            "Epoch 466, training loss: 0.014543\n",
            "Epoch 467, training loss: 0.014538\n",
            "Epoch 468, training loss: 0.014533\n",
            "Epoch 469, training loss: 0.014528\n",
            "Epoch 470, training loss: 0.014522\n",
            "Epoch 471, training loss: 0.014517\n",
            "Epoch 472, training loss: 0.014512\n",
            "Epoch 473, training loss: 0.014507\n",
            "Epoch 474, training loss: 0.014502\n",
            "Epoch 475, training loss: 0.014497\n",
            "Epoch 476, training loss: 0.014492\n",
            "Epoch 477, training loss: 0.014487\n",
            "Epoch 478, training loss: 0.014483\n",
            "Epoch 479, training loss: 0.014478\n",
            "Epoch 480, training loss: 0.014473\n",
            "Epoch 481, training loss: 0.014468\n",
            "Epoch 482, training loss: 0.014463\n",
            "Epoch 483, training loss: 0.014458\n",
            "Epoch 484, training loss: 0.014454\n",
            "Epoch 485, training loss: 0.014449\n",
            "Epoch 486, training loss: 0.014444\n",
            "Epoch 487, training loss: 0.014440\n",
            "Epoch 488, training loss: 0.014435\n",
            "Epoch 489, training loss: 0.014430\n",
            "Epoch 490, training loss: 0.014426\n",
            "Epoch 491, training loss: 0.014421\n",
            "Epoch 492, training loss: 0.014416\n",
            "Epoch 493, training loss: 0.014412\n",
            "Epoch 494, training loss: 0.014407\n",
            "Epoch 495, training loss: 0.014403\n",
            "Epoch 496, training loss: 0.014398\n",
            "Epoch 497, training loss: 0.014394\n",
            "Epoch 498, training loss: 0.014389\n",
            "Epoch 499, training loss: 0.014385\n",
            "Epoch 0, testing loss: 0.024462\n",
            "Epoch 1, testing loss: 0.024462\n",
            "Epoch 2, testing loss: 0.024462\n",
            "Epoch 3, testing loss: 0.024462\n",
            "Epoch 4, testing loss: 0.024462\n",
            "Epoch 5, testing loss: 0.024462\n",
            "Epoch 6, testing loss: 0.024462\n",
            "Epoch 7, testing loss: 0.024462\n",
            "Epoch 8, testing loss: 0.024462\n",
            "Epoch 9, testing loss: 0.024462\n",
            "Epoch 10, testing loss: 0.024462\n",
            "Epoch 11, testing loss: 0.024462\n",
            "Epoch 12, testing loss: 0.024462\n",
            "Epoch 13, testing loss: 0.024462\n",
            "Epoch 14, testing loss: 0.024462\n",
            "Epoch 15, testing loss: 0.024462\n",
            "Epoch 16, testing loss: 0.024462\n",
            "Epoch 17, testing loss: 0.024462\n",
            "Epoch 18, testing loss: 0.024462\n",
            "Epoch 19, testing loss: 0.024462\n",
            "Epoch 20, testing loss: 0.024462\n",
            "Epoch 21, testing loss: 0.024462\n",
            "Epoch 22, testing loss: 0.024462\n",
            "Epoch 23, testing loss: 0.024462\n",
            "Epoch 24, testing loss: 0.024462\n",
            "Epoch 25, testing loss: 0.024462\n",
            "Epoch 26, testing loss: 0.024462\n",
            "Epoch 27, testing loss: 0.024462\n",
            "Epoch 28, testing loss: 0.024462\n",
            "Epoch 29, testing loss: 0.024462\n",
            "Epoch 30, testing loss: 0.024462\n",
            "Epoch 31, testing loss: 0.024462\n",
            "Epoch 32, testing loss: 0.024462\n",
            "Epoch 33, testing loss: 0.024462\n",
            "Epoch 34, testing loss: 0.024462\n",
            "Epoch 35, testing loss: 0.024462\n",
            "Epoch 36, testing loss: 0.024462\n",
            "Epoch 37, testing loss: 0.024462\n",
            "Epoch 38, testing loss: 0.024462\n",
            "Epoch 39, testing loss: 0.024462\n",
            "Epoch 40, testing loss: 0.024462\n",
            "Epoch 41, testing loss: 0.024462\n",
            "Epoch 42, testing loss: 0.024462\n",
            "Epoch 43, testing loss: 0.024462\n",
            "Epoch 44, testing loss: 0.024462\n",
            "Epoch 45, testing loss: 0.024462\n",
            "Epoch 46, testing loss: 0.024462\n",
            "Epoch 47, testing loss: 0.024462\n",
            "Epoch 48, testing loss: 0.024462\n",
            "Epoch 49, testing loss: 0.024462\n",
            "Epoch 50, testing loss: 0.024462\n",
            "Epoch 51, testing loss: 0.024462\n",
            "Epoch 52, testing loss: 0.024462\n",
            "Epoch 53, testing loss: 0.024462\n",
            "Epoch 54, testing loss: 0.024462\n",
            "Epoch 55, testing loss: 0.024462\n",
            "Epoch 56, testing loss: 0.024462\n",
            "Epoch 57, testing loss: 0.024462\n",
            "Epoch 58, testing loss: 0.024462\n",
            "Epoch 59, testing loss: 0.024462\n",
            "Epoch 60, testing loss: 0.024462\n",
            "Epoch 61, testing loss: 0.024462\n",
            "Epoch 62, testing loss: 0.024462\n",
            "Epoch 63, testing loss: 0.024462\n",
            "Epoch 64, testing loss: 0.024462\n",
            "Epoch 65, testing loss: 0.024462\n",
            "Epoch 66, testing loss: 0.024462\n",
            "Epoch 67, testing loss: 0.024462\n",
            "Epoch 68, testing loss: 0.024462\n",
            "Epoch 69, testing loss: 0.024462\n",
            "Epoch 70, testing loss: 0.024462\n",
            "Epoch 71, testing loss: 0.024462\n",
            "Epoch 72, testing loss: 0.024462\n",
            "Epoch 73, testing loss: 0.024462\n",
            "Epoch 74, testing loss: 0.024462\n",
            "Epoch 75, testing loss: 0.024462\n",
            "Epoch 76, testing loss: 0.024462\n",
            "Epoch 77, testing loss: 0.024462\n",
            "Epoch 78, testing loss: 0.024462\n",
            "Epoch 79, testing loss: 0.024462\n",
            "Epoch 80, testing loss: 0.024462\n",
            "Epoch 81, testing loss: 0.024462\n",
            "Epoch 82, testing loss: 0.024462\n",
            "Epoch 83, testing loss: 0.024462\n",
            "Epoch 84, testing loss: 0.024462\n",
            "Epoch 85, testing loss: 0.024462\n",
            "Epoch 86, testing loss: 0.024462\n",
            "Epoch 87, testing loss: 0.024462\n",
            "Epoch 88, testing loss: 0.024462\n",
            "Epoch 89, testing loss: 0.024462\n",
            "Epoch 90, testing loss: 0.024462\n",
            "Epoch 91, testing loss: 0.024462\n",
            "Epoch 92, testing loss: 0.024462\n",
            "Epoch 93, testing loss: 0.024462\n",
            "Epoch 94, testing loss: 0.024462\n",
            "Epoch 95, testing loss: 0.024462\n",
            "Epoch 96, testing loss: 0.024462\n",
            "Epoch 97, testing loss: 0.024462\n",
            "Epoch 98, testing loss: 0.024462\n",
            "Epoch 99, testing loss: 0.024462\n",
            "Epoch 100, testing loss: 0.024462\n",
            "Epoch 101, testing loss: 0.024462\n",
            "Epoch 102, testing loss: 0.024462\n",
            "Epoch 103, testing loss: 0.024462\n",
            "Epoch 104, testing loss: 0.024462\n",
            "Epoch 105, testing loss: 0.024462\n",
            "Epoch 106, testing loss: 0.024462\n",
            "Epoch 107, testing loss: 0.024462\n",
            "Epoch 108, testing loss: 0.024462\n",
            "Epoch 109, testing loss: 0.024462\n",
            "Epoch 110, testing loss: 0.024462\n",
            "Epoch 111, testing loss: 0.024462\n",
            "Epoch 112, testing loss: 0.024462\n",
            "Epoch 113, testing loss: 0.024462\n",
            "Epoch 114, testing loss: 0.024462\n",
            "Epoch 115, testing loss: 0.024462\n",
            "Epoch 116, testing loss: 0.024462\n",
            "Epoch 117, testing loss: 0.024462\n",
            "Epoch 118, testing loss: 0.024462\n",
            "Epoch 119, testing loss: 0.024462\n",
            "Epoch 120, testing loss: 0.024462\n",
            "Epoch 121, testing loss: 0.024462\n",
            "Epoch 122, testing loss: 0.024462\n",
            "Epoch 123, testing loss: 0.024462\n",
            "Epoch 124, testing loss: 0.024462\n",
            "Epoch 125, testing loss: 0.024462\n",
            "Epoch 126, testing loss: 0.024462\n",
            "Epoch 127, testing loss: 0.024462\n",
            "Epoch 128, testing loss: 0.024462\n",
            "Epoch 129, testing loss: 0.024462\n",
            "Epoch 130, testing loss: 0.024462\n",
            "Epoch 131, testing loss: 0.024462\n",
            "Epoch 132, testing loss: 0.024462\n",
            "Epoch 133, testing loss: 0.024462\n",
            "Epoch 134, testing loss: 0.024462\n",
            "Epoch 135, testing loss: 0.024462\n",
            "Epoch 136, testing loss: 0.024462\n",
            "Epoch 137, testing loss: 0.024462\n",
            "Epoch 138, testing loss: 0.024462\n",
            "Epoch 139, testing loss: 0.024462\n",
            "Epoch 140, testing loss: 0.024462\n",
            "Epoch 141, testing loss: 0.024462\n",
            "Epoch 142, testing loss: 0.024462\n",
            "Epoch 143, testing loss: 0.024462\n",
            "Epoch 144, testing loss: 0.024462\n",
            "Epoch 145, testing loss: 0.024462\n",
            "Epoch 146, testing loss: 0.024462\n",
            "Epoch 147, testing loss: 0.024462\n",
            "Epoch 148, testing loss: 0.024462\n",
            "Epoch 149, testing loss: 0.024462\n",
            "Epoch 150, testing loss: 0.024462\n",
            "Epoch 151, testing loss: 0.024462\n",
            "Epoch 152, testing loss: 0.024462\n",
            "Epoch 153, testing loss: 0.024462\n",
            "Epoch 154, testing loss: 0.024462\n",
            "Epoch 155, testing loss: 0.024462\n",
            "Epoch 156, testing loss: 0.024462\n",
            "Epoch 157, testing loss: 0.024462\n",
            "Epoch 158, testing loss: 0.024462\n",
            "Epoch 159, testing loss: 0.024462\n",
            "Epoch 160, testing loss: 0.024462\n",
            "Epoch 161, testing loss: 0.024462\n",
            "Epoch 162, testing loss: 0.024462\n",
            "Epoch 163, testing loss: 0.024462\n",
            "Epoch 164, testing loss: 0.024462\n",
            "Epoch 165, testing loss: 0.024462\n",
            "Epoch 166, testing loss: 0.024462\n",
            "Epoch 167, testing loss: 0.024462\n",
            "Epoch 168, testing loss: 0.024462\n",
            "Epoch 169, testing loss: 0.024462\n",
            "Epoch 170, testing loss: 0.024462\n",
            "Epoch 171, testing loss: 0.024462\n",
            "Epoch 172, testing loss: 0.024462\n",
            "Epoch 173, testing loss: 0.024462\n",
            "Epoch 174, testing loss: 0.024462\n",
            "Epoch 175, testing loss: 0.024462\n",
            "Epoch 176, testing loss: 0.024462\n",
            "Epoch 177, testing loss: 0.024462\n",
            "Epoch 178, testing loss: 0.024462\n",
            "Epoch 179, testing loss: 0.024462\n",
            "Epoch 180, testing loss: 0.024462\n",
            "Epoch 181, testing loss: 0.024462\n",
            "Epoch 182, testing loss: 0.024462\n",
            "Epoch 183, testing loss: 0.024462\n",
            "Epoch 184, testing loss: 0.024462\n",
            "Epoch 185, testing loss: 0.024462\n",
            "Epoch 186, testing loss: 0.024462\n",
            "Epoch 187, testing loss: 0.024462\n",
            "Epoch 188, testing loss: 0.024462\n",
            "Epoch 189, testing loss: 0.024462\n",
            "Epoch 190, testing loss: 0.024462\n",
            "Epoch 191, testing loss: 0.024462\n",
            "Epoch 192, testing loss: 0.024462\n",
            "Epoch 193, testing loss: 0.024462\n",
            "Epoch 194, testing loss: 0.024462\n",
            "Epoch 195, testing loss: 0.024462\n",
            "Epoch 196, testing loss: 0.024462\n",
            "Epoch 197, testing loss: 0.024462\n",
            "Epoch 198, testing loss: 0.024462\n",
            "Epoch 199, testing loss: 0.024462\n",
            "Epoch 200, testing loss: 0.024462\n",
            "Epoch 201, testing loss: 0.024462\n",
            "Epoch 202, testing loss: 0.024462\n",
            "Epoch 203, testing loss: 0.024462\n",
            "Epoch 204, testing loss: 0.024462\n",
            "Epoch 205, testing loss: 0.024462\n",
            "Epoch 206, testing loss: 0.024462\n",
            "Epoch 207, testing loss: 0.024462\n",
            "Epoch 208, testing loss: 0.024462\n",
            "Epoch 209, testing loss: 0.024462\n",
            "Epoch 210, testing loss: 0.024462\n",
            "Epoch 211, testing loss: 0.024462\n",
            "Epoch 212, testing loss: 0.024462\n",
            "Epoch 213, testing loss: 0.024462\n",
            "Epoch 214, testing loss: 0.024462\n",
            "Epoch 215, testing loss: 0.024462\n",
            "Epoch 216, testing loss: 0.024462\n",
            "Epoch 217, testing loss: 0.024462\n",
            "Epoch 218, testing loss: 0.024462\n",
            "Epoch 219, testing loss: 0.024462\n",
            "Epoch 220, testing loss: 0.024462\n",
            "Epoch 221, testing loss: 0.024462\n",
            "Epoch 222, testing loss: 0.024462\n",
            "Epoch 223, testing loss: 0.024462\n",
            "Epoch 224, testing loss: 0.024462\n",
            "Epoch 225, testing loss: 0.024462\n",
            "Epoch 226, testing loss: 0.024462\n",
            "Epoch 227, testing loss: 0.024462\n",
            "Epoch 228, testing loss: 0.024462\n",
            "Epoch 229, testing loss: 0.024462\n",
            "Epoch 230, testing loss: 0.024462\n",
            "Epoch 231, testing loss: 0.024462\n",
            "Epoch 232, testing loss: 0.024462\n",
            "Epoch 233, testing loss: 0.024462\n",
            "Epoch 234, testing loss: 0.024462\n",
            "Epoch 235, testing loss: 0.024462\n",
            "Epoch 236, testing loss: 0.024462\n",
            "Epoch 237, testing loss: 0.024462\n",
            "Epoch 238, testing loss: 0.024462\n",
            "Epoch 239, testing loss: 0.024462\n",
            "Epoch 240, testing loss: 0.024462\n",
            "Epoch 241, testing loss: 0.024462\n",
            "Epoch 242, testing loss: 0.024462\n",
            "Epoch 243, testing loss: 0.024462\n",
            "Epoch 244, testing loss: 0.024462\n",
            "Epoch 245, testing loss: 0.024462\n",
            "Epoch 246, testing loss: 0.024462\n",
            "Epoch 247, testing loss: 0.024462\n",
            "Epoch 248, testing loss: 0.024462\n",
            "Epoch 249, testing loss: 0.024462\n",
            "Epoch 250, testing loss: 0.024462\n",
            "Epoch 251, testing loss: 0.024462\n",
            "Epoch 252, testing loss: 0.024462\n",
            "Epoch 253, testing loss: 0.024462\n",
            "Epoch 254, testing loss: 0.024462\n",
            "Epoch 255, testing loss: 0.024462\n",
            "Epoch 256, testing loss: 0.024462\n",
            "Epoch 257, testing loss: 0.024462\n",
            "Epoch 258, testing loss: 0.024462\n",
            "Epoch 259, testing loss: 0.024462\n",
            "Epoch 260, testing loss: 0.024462\n",
            "Epoch 261, testing loss: 0.024462\n",
            "Epoch 262, testing loss: 0.024462\n",
            "Epoch 263, testing loss: 0.024462\n",
            "Epoch 264, testing loss: 0.024462\n",
            "Epoch 265, testing loss: 0.024462\n",
            "Epoch 266, testing loss: 0.024462\n",
            "Epoch 267, testing loss: 0.024462\n",
            "Epoch 268, testing loss: 0.024462\n",
            "Epoch 269, testing loss: 0.024462\n",
            "Epoch 270, testing loss: 0.024462\n",
            "Epoch 271, testing loss: 0.024462\n",
            "Epoch 272, testing loss: 0.024462\n",
            "Epoch 273, testing loss: 0.024462\n",
            "Epoch 274, testing loss: 0.024462\n",
            "Epoch 275, testing loss: 0.024462\n",
            "Epoch 276, testing loss: 0.024462\n",
            "Epoch 277, testing loss: 0.024462\n",
            "Epoch 278, testing loss: 0.024462\n",
            "Epoch 279, testing loss: 0.024462\n",
            "Epoch 280, testing loss: 0.024462\n",
            "Epoch 281, testing loss: 0.024462\n",
            "Epoch 282, testing loss: 0.024462\n",
            "Epoch 283, testing loss: 0.024462\n",
            "Epoch 284, testing loss: 0.024462\n",
            "Epoch 285, testing loss: 0.024462\n",
            "Epoch 286, testing loss: 0.024462\n",
            "Epoch 287, testing loss: 0.024462\n",
            "Epoch 288, testing loss: 0.024462\n",
            "Epoch 289, testing loss: 0.024462\n",
            "Epoch 290, testing loss: 0.024462\n",
            "Epoch 291, testing loss: 0.024462\n",
            "Epoch 292, testing loss: 0.024462\n",
            "Epoch 293, testing loss: 0.024462\n",
            "Epoch 294, testing loss: 0.024462\n",
            "Epoch 295, testing loss: 0.024462\n",
            "Epoch 296, testing loss: 0.024462\n",
            "Epoch 297, testing loss: 0.024462\n",
            "Epoch 298, testing loss: 0.024462\n",
            "Epoch 299, testing loss: 0.024462\n",
            "Epoch 300, testing loss: 0.024462\n",
            "Epoch 301, testing loss: 0.024462\n",
            "Epoch 302, testing loss: 0.024462\n",
            "Epoch 303, testing loss: 0.024462\n",
            "Epoch 304, testing loss: 0.024462\n",
            "Epoch 305, testing loss: 0.024462\n",
            "Epoch 306, testing loss: 0.024462\n",
            "Epoch 307, testing loss: 0.024462\n",
            "Epoch 308, testing loss: 0.024462\n",
            "Epoch 309, testing loss: 0.024462\n",
            "Epoch 310, testing loss: 0.024462\n",
            "Epoch 311, testing loss: 0.024462\n",
            "Epoch 312, testing loss: 0.024462\n",
            "Epoch 313, testing loss: 0.024462\n",
            "Epoch 314, testing loss: 0.024462\n",
            "Epoch 315, testing loss: 0.024462\n",
            "Epoch 316, testing loss: 0.024462\n",
            "Epoch 317, testing loss: 0.024462\n",
            "Epoch 318, testing loss: 0.024462\n",
            "Epoch 319, testing loss: 0.024462\n",
            "Epoch 320, testing loss: 0.024462\n",
            "Epoch 321, testing loss: 0.024462\n",
            "Epoch 322, testing loss: 0.024462\n",
            "Epoch 323, testing loss: 0.024462\n",
            "Epoch 324, testing loss: 0.024462\n",
            "Epoch 325, testing loss: 0.024462\n",
            "Epoch 326, testing loss: 0.024462\n",
            "Epoch 327, testing loss: 0.024462\n",
            "Epoch 328, testing loss: 0.024462\n",
            "Epoch 329, testing loss: 0.024462\n",
            "Epoch 330, testing loss: 0.024462\n",
            "Epoch 331, testing loss: 0.024462\n",
            "Epoch 332, testing loss: 0.024462\n",
            "Epoch 333, testing loss: 0.024462\n",
            "Epoch 334, testing loss: 0.024462\n",
            "Epoch 335, testing loss: 0.024462\n",
            "Epoch 336, testing loss: 0.024462\n",
            "Epoch 337, testing loss: 0.024462\n",
            "Epoch 338, testing loss: 0.024462\n",
            "Epoch 339, testing loss: 0.024462\n",
            "Epoch 340, testing loss: 0.024462\n",
            "Epoch 341, testing loss: 0.024462\n",
            "Epoch 342, testing loss: 0.024462\n",
            "Epoch 343, testing loss: 0.024462\n",
            "Epoch 344, testing loss: 0.024462\n",
            "Epoch 345, testing loss: 0.024462\n",
            "Epoch 346, testing loss: 0.024462\n",
            "Epoch 347, testing loss: 0.024462\n",
            "Epoch 348, testing loss: 0.024462\n",
            "Epoch 349, testing loss: 0.024462\n",
            "Epoch 350, testing loss: 0.024462\n",
            "Epoch 351, testing loss: 0.024462\n",
            "Epoch 352, testing loss: 0.024462\n",
            "Epoch 353, testing loss: 0.024462\n",
            "Epoch 354, testing loss: 0.024462\n",
            "Epoch 355, testing loss: 0.024462\n",
            "Epoch 356, testing loss: 0.024462\n",
            "Epoch 357, testing loss: 0.024462\n",
            "Epoch 358, testing loss: 0.024462\n",
            "Epoch 359, testing loss: 0.024462\n",
            "Epoch 360, testing loss: 0.024462\n",
            "Epoch 361, testing loss: 0.024462\n",
            "Epoch 362, testing loss: 0.024462\n",
            "Epoch 363, testing loss: 0.024462\n",
            "Epoch 364, testing loss: 0.024462\n",
            "Epoch 365, testing loss: 0.024462\n",
            "Epoch 366, testing loss: 0.024462\n",
            "Epoch 367, testing loss: 0.024462\n",
            "Epoch 368, testing loss: 0.024462\n",
            "Epoch 369, testing loss: 0.024462\n",
            "Epoch 370, testing loss: 0.024462\n",
            "Epoch 371, testing loss: 0.024462\n",
            "Epoch 372, testing loss: 0.024462\n",
            "Epoch 373, testing loss: 0.024462\n",
            "Epoch 374, testing loss: 0.024462\n",
            "Epoch 375, testing loss: 0.024462\n",
            "Epoch 376, testing loss: 0.024462\n",
            "Epoch 377, testing loss: 0.024462\n",
            "Epoch 378, testing loss: 0.024462\n",
            "Epoch 379, testing loss: 0.024462\n",
            "Epoch 380, testing loss: 0.024462\n",
            "Epoch 381, testing loss: 0.024462\n",
            "Epoch 382, testing loss: 0.024462\n",
            "Epoch 383, testing loss: 0.024462\n",
            "Epoch 384, testing loss: 0.024462\n",
            "Epoch 385, testing loss: 0.024462\n",
            "Epoch 386, testing loss: 0.024462\n",
            "Epoch 387, testing loss: 0.024462\n",
            "Epoch 388, testing loss: 0.024462\n",
            "Epoch 389, testing loss: 0.024462\n",
            "Epoch 390, testing loss: 0.024462\n",
            "Epoch 391, testing loss: 0.024462\n",
            "Epoch 392, testing loss: 0.024462\n",
            "Epoch 393, testing loss: 0.024462\n",
            "Epoch 394, testing loss: 0.024462\n",
            "Epoch 395, testing loss: 0.024462\n",
            "Epoch 396, testing loss: 0.024462\n",
            "Epoch 397, testing loss: 0.024462\n",
            "Epoch 398, testing loss: 0.024462\n",
            "Epoch 399, testing loss: 0.024462\n",
            "Epoch 400, testing loss: 0.024462\n",
            "Epoch 401, testing loss: 0.024462\n",
            "Epoch 402, testing loss: 0.024462\n",
            "Epoch 403, testing loss: 0.024462\n",
            "Epoch 404, testing loss: 0.024462\n",
            "Epoch 405, testing loss: 0.024462\n",
            "Epoch 406, testing loss: 0.024462\n",
            "Epoch 407, testing loss: 0.024462\n",
            "Epoch 408, testing loss: 0.024462\n",
            "Epoch 409, testing loss: 0.024462\n",
            "Epoch 410, testing loss: 0.024462\n",
            "Epoch 411, testing loss: 0.024462\n",
            "Epoch 412, testing loss: 0.024462\n",
            "Epoch 413, testing loss: 0.024462\n",
            "Epoch 414, testing loss: 0.024462\n",
            "Epoch 415, testing loss: 0.024462\n",
            "Epoch 416, testing loss: 0.024462\n",
            "Epoch 417, testing loss: 0.024462\n",
            "Epoch 418, testing loss: 0.024462\n",
            "Epoch 419, testing loss: 0.024462\n",
            "Epoch 420, testing loss: 0.024462\n",
            "Epoch 421, testing loss: 0.024462\n",
            "Epoch 422, testing loss: 0.024462\n",
            "Epoch 423, testing loss: 0.024462\n",
            "Epoch 424, testing loss: 0.024462\n",
            "Epoch 425, testing loss: 0.024462\n",
            "Epoch 426, testing loss: 0.024462\n",
            "Epoch 427, testing loss: 0.024462\n",
            "Epoch 428, testing loss: 0.024462\n",
            "Epoch 429, testing loss: 0.024462\n",
            "Epoch 430, testing loss: 0.024462\n",
            "Epoch 431, testing loss: 0.024462\n",
            "Epoch 432, testing loss: 0.024462\n",
            "Epoch 433, testing loss: 0.024462\n",
            "Epoch 434, testing loss: 0.024462\n",
            "Epoch 435, testing loss: 0.024462\n",
            "Epoch 436, testing loss: 0.024462\n",
            "Epoch 437, testing loss: 0.024462\n",
            "Epoch 438, testing loss: 0.024462\n",
            "Epoch 439, testing loss: 0.024462\n",
            "Epoch 440, testing loss: 0.024462\n",
            "Epoch 441, testing loss: 0.024462\n",
            "Epoch 442, testing loss: 0.024462\n",
            "Epoch 443, testing loss: 0.024462\n",
            "Epoch 444, testing loss: 0.024462\n",
            "Epoch 445, testing loss: 0.024462\n",
            "Epoch 446, testing loss: 0.024462\n",
            "Epoch 447, testing loss: 0.024462\n",
            "Epoch 448, testing loss: 0.024462\n",
            "Epoch 449, testing loss: 0.024462\n",
            "Epoch 450, testing loss: 0.024462\n",
            "Epoch 451, testing loss: 0.024462\n",
            "Epoch 452, testing loss: 0.024462\n",
            "Epoch 453, testing loss: 0.024462\n",
            "Epoch 454, testing loss: 0.024462\n",
            "Epoch 455, testing loss: 0.024462\n",
            "Epoch 456, testing loss: 0.024462\n",
            "Epoch 457, testing loss: 0.024462\n",
            "Epoch 458, testing loss: 0.024462\n",
            "Epoch 459, testing loss: 0.024462\n",
            "Epoch 460, testing loss: 0.024462\n",
            "Epoch 461, testing loss: 0.024462\n",
            "Epoch 462, testing loss: 0.024462\n",
            "Epoch 463, testing loss: 0.024462\n",
            "Epoch 464, testing loss: 0.024462\n",
            "Epoch 465, testing loss: 0.024462\n",
            "Epoch 466, testing loss: 0.024462\n",
            "Epoch 467, testing loss: 0.024462\n",
            "Epoch 468, testing loss: 0.024462\n",
            "Epoch 469, testing loss: 0.024462\n",
            "Epoch 470, testing loss: 0.024462\n",
            "Epoch 471, testing loss: 0.024462\n",
            "Epoch 472, testing loss: 0.024462\n",
            "Epoch 473, testing loss: 0.024462\n",
            "Epoch 474, testing loss: 0.024462\n",
            "Epoch 475, testing loss: 0.024462\n",
            "Epoch 476, testing loss: 0.024462\n",
            "Epoch 477, testing loss: 0.024462\n",
            "Epoch 478, testing loss: 0.024462\n",
            "Epoch 479, testing loss: 0.024462\n",
            "Epoch 480, testing loss: 0.024462\n",
            "Epoch 481, testing loss: 0.024462\n",
            "Epoch 482, testing loss: 0.024462\n",
            "Epoch 483, testing loss: 0.024462\n",
            "Epoch 484, testing loss: 0.024462\n",
            "Epoch 485, testing loss: 0.024462\n",
            "Epoch 486, testing loss: 0.024462\n",
            "Epoch 487, testing loss: 0.024462\n",
            "Epoch 488, testing loss: 0.024462\n",
            "Epoch 489, testing loss: 0.024462\n",
            "Epoch 490, testing loss: 0.024462\n",
            "Epoch 491, testing loss: 0.024462\n",
            "Epoch 492, testing loss: 0.024462\n",
            "Epoch 493, testing loss: 0.024462\n",
            "Epoch 494, testing loss: 0.024462\n",
            "Epoch 495, testing loss: 0.024462\n",
            "Epoch 496, testing loss: 0.024462\n",
            "Epoch 497, testing loss: 0.024462\n",
            "Epoch 498, testing loss: 0.024462\n",
            "Epoch 499, testing loss: 0.024462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model Framwework Training and Evaluation"
      ],
      "metadata": {
        "id": "QT5C3G-F5yF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(500):\n",
        "    with autograd.record():\n",
        "        output = net(train_data)\n",
        "        L = loss(output, train_labels)\n",
        "    L.backward()\n",
        "    trainer.step(train_data.shape[0])\n",
        "    print('Epoch %d, training loss: %f' % (epoch, mx.nd.mean(L).asscalar()))\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "for epoch in range(500):\n",
        "    test_output = net(test_data)\n",
        "    test_loss = loss(test_output, test_labels)\n",
        "    print('Epoch %d, testing loss: %f' % (epoch, mx.nd.mean(test_loss).asscalar()))"
      ],
      "metadata": {
        "id": "zNnzhtBW50ua",
        "outputId": "05a41816-0213-4cff-fe3b-6bd633b3a7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, training loss: 0.014381\n",
            "Epoch 1, training loss: 0.014376\n",
            "Epoch 2, training loss: 0.014372\n",
            "Epoch 3, training loss: 0.014367\n",
            "Epoch 4, training loss: 0.014363\n",
            "Epoch 5, training loss: 0.014359\n",
            "Epoch 6, training loss: 0.014355\n",
            "Epoch 7, training loss: 0.014350\n",
            "Epoch 8, training loss: 0.014346\n",
            "Epoch 9, training loss: 0.014342\n",
            "Epoch 10, training loss: 0.014338\n",
            "Epoch 11, training loss: 0.014333\n",
            "Epoch 12, training loss: 0.014329\n",
            "Epoch 13, training loss: 0.014325\n",
            "Epoch 14, training loss: 0.014321\n",
            "Epoch 15, training loss: 0.014317\n",
            "Epoch 16, training loss: 0.014313\n",
            "Epoch 17, training loss: 0.014309\n",
            "Epoch 18, training loss: 0.014305\n",
            "Epoch 19, training loss: 0.014301\n",
            "Epoch 20, training loss: 0.014297\n",
            "Epoch 21, training loss: 0.014293\n",
            "Epoch 22, training loss: 0.014289\n",
            "Epoch 23, training loss: 0.014285\n",
            "Epoch 24, training loss: 0.014281\n",
            "Epoch 25, training loss: 0.014277\n",
            "Epoch 26, training loss: 0.014273\n",
            "Epoch 27, training loss: 0.014269\n",
            "Epoch 28, training loss: 0.014265\n",
            "Epoch 29, training loss: 0.014262\n",
            "Epoch 30, training loss: 0.014258\n",
            "Epoch 31, training loss: 0.014254\n",
            "Epoch 32, training loss: 0.014250\n",
            "Epoch 33, training loss: 0.014246\n",
            "Epoch 34, training loss: 0.014242\n",
            "Epoch 35, training loss: 0.014239\n",
            "Epoch 36, training loss: 0.014235\n",
            "Epoch 37, training loss: 0.014231\n",
            "Epoch 38, training loss: 0.014227\n",
            "Epoch 39, training loss: 0.014224\n",
            "Epoch 40, training loss: 0.014220\n",
            "Epoch 41, training loss: 0.014216\n",
            "Epoch 42, training loss: 0.014213\n",
            "Epoch 43, training loss: 0.014209\n",
            "Epoch 44, training loss: 0.014205\n",
            "Epoch 45, training loss: 0.014202\n",
            "Epoch 46, training loss: 0.014198\n",
            "Epoch 47, training loss: 0.014194\n",
            "Epoch 48, training loss: 0.014191\n",
            "Epoch 49, training loss: 0.014187\n",
            "Epoch 50, training loss: 0.014184\n",
            "Epoch 51, training loss: 0.014180\n",
            "Epoch 52, training loss: 0.014177\n",
            "Epoch 53, training loss: 0.014173\n",
            "Epoch 54, training loss: 0.014170\n",
            "Epoch 55, training loss: 0.014166\n",
            "Epoch 56, training loss: 0.014163\n",
            "Epoch 57, training loss: 0.014159\n",
            "Epoch 58, training loss: 0.014156\n",
            "Epoch 59, training loss: 0.014153\n",
            "Epoch 60, training loss: 0.014149\n",
            "Epoch 61, training loss: 0.014146\n",
            "Epoch 62, training loss: 0.014143\n",
            "Epoch 63, training loss: 0.014139\n",
            "Epoch 64, training loss: 0.014136\n",
            "Epoch 65, training loss: 0.014133\n",
            "Epoch 66, training loss: 0.014129\n",
            "Epoch 67, training loss: 0.014126\n",
            "Epoch 68, training loss: 0.014123\n",
            "Epoch 69, training loss: 0.014120\n",
            "Epoch 70, training loss: 0.014116\n",
            "Epoch 71, training loss: 0.014113\n",
            "Epoch 72, training loss: 0.014110\n",
            "Epoch 73, training loss: 0.014107\n",
            "Epoch 74, training loss: 0.014104\n",
            "Epoch 75, training loss: 0.014100\n",
            "Epoch 76, training loss: 0.014097\n",
            "Epoch 77, training loss: 0.014094\n",
            "Epoch 78, training loss: 0.014091\n",
            "Epoch 79, training loss: 0.014088\n",
            "Epoch 80, training loss: 0.014085\n",
            "Epoch 81, training loss: 0.014082\n",
            "Epoch 82, training loss: 0.014079\n",
            "Epoch 83, training loss: 0.014076\n",
            "Epoch 84, training loss: 0.014073\n",
            "Epoch 85, training loss: 0.014070\n",
            "Epoch 86, training loss: 0.014067\n",
            "Epoch 87, training loss: 0.014064\n",
            "Epoch 88, training loss: 0.014061\n",
            "Epoch 89, training loss: 0.014058\n",
            "Epoch 90, training loss: 0.014055\n",
            "Epoch 91, training loss: 0.014052\n",
            "Epoch 92, training loss: 0.014049\n",
            "Epoch 93, training loss: 0.014046\n",
            "Epoch 94, training loss: 0.014043\n",
            "Epoch 95, training loss: 0.014040\n",
            "Epoch 96, training loss: 0.014037\n",
            "Epoch 97, training loss: 0.014034\n",
            "Epoch 98, training loss: 0.014032\n",
            "Epoch 99, training loss: 0.014029\n",
            "Epoch 100, training loss: 0.014026\n",
            "Epoch 101, training loss: 0.014023\n",
            "Epoch 102, training loss: 0.014020\n",
            "Epoch 103, training loss: 0.014017\n",
            "Epoch 104, training loss: 0.014015\n",
            "Epoch 105, training loss: 0.014012\n",
            "Epoch 106, training loss: 0.014009\n",
            "Epoch 107, training loss: 0.014006\n",
            "Epoch 108, training loss: 0.014004\n",
            "Epoch 109, training loss: 0.014001\n",
            "Epoch 110, training loss: 0.013998\n",
            "Epoch 111, training loss: 0.013995\n",
            "Epoch 112, training loss: 0.013993\n",
            "Epoch 113, training loss: 0.013990\n",
            "Epoch 114, training loss: 0.013987\n",
            "Epoch 115, training loss: 0.013985\n",
            "Epoch 116, training loss: 0.013982\n",
            "Epoch 117, training loss: 0.013979\n",
            "Epoch 118, training loss: 0.013977\n",
            "Epoch 119, training loss: 0.013974\n",
            "Epoch 120, training loss: 0.013972\n",
            "Epoch 121, training loss: 0.013969\n",
            "Epoch 122, training loss: 0.013966\n",
            "Epoch 123, training loss: 0.013964\n",
            "Epoch 124, training loss: 0.013961\n",
            "Epoch 125, training loss: 0.013959\n",
            "Epoch 126, training loss: 0.013956\n",
            "Epoch 127, training loss: 0.013954\n",
            "Epoch 128, training loss: 0.013951\n",
            "Epoch 129, training loss: 0.013949\n",
            "Epoch 130, training loss: 0.013947\n",
            "Epoch 131, training loss: 0.013944\n",
            "Epoch 132, training loss: 0.013942\n",
            "Epoch 133, training loss: 0.013939\n",
            "Epoch 134, training loss: 0.013937\n",
            "Epoch 135, training loss: 0.013935\n",
            "Epoch 136, training loss: 0.013932\n",
            "Epoch 137, training loss: 0.013930\n",
            "Epoch 138, training loss: 0.013928\n",
            "Epoch 139, training loss: 0.013926\n",
            "Epoch 140, training loss: 0.013923\n",
            "Epoch 141, training loss: 0.013921\n",
            "Epoch 142, training loss: 0.013919\n",
            "Epoch 143, training loss: 0.013917\n",
            "Epoch 144, training loss: 0.013914\n",
            "Epoch 145, training loss: 0.013912\n",
            "Epoch 146, training loss: 0.013910\n",
            "Epoch 147, training loss: 0.013908\n",
            "Epoch 148, training loss: 0.013905\n",
            "Epoch 149, training loss: 0.013903\n",
            "Epoch 150, training loss: 0.013901\n",
            "Epoch 151, training loss: 0.013899\n",
            "Epoch 152, training loss: 0.013897\n",
            "Epoch 153, training loss: 0.013895\n",
            "Epoch 154, training loss: 0.013892\n",
            "Epoch 155, training loss: 0.013890\n",
            "Epoch 156, training loss: 0.013888\n",
            "Epoch 157, training loss: 0.013886\n",
            "Epoch 158, training loss: 0.013884\n",
            "Epoch 159, training loss: 0.013882\n",
            "Epoch 160, training loss: 0.013880\n",
            "Epoch 161, training loss: 0.013878\n",
            "Epoch 162, training loss: 0.013876\n",
            "Epoch 163, training loss: 0.013873\n",
            "Epoch 164, training loss: 0.013871\n",
            "Epoch 165, training loss: 0.013869\n",
            "Epoch 166, training loss: 0.013867\n",
            "Epoch 167, training loss: 0.013865\n",
            "Epoch 168, training loss: 0.013863\n",
            "Epoch 169, training loss: 0.013861\n",
            "Epoch 170, training loss: 0.013859\n",
            "Epoch 171, training loss: 0.013857\n",
            "Epoch 172, training loss: 0.013855\n",
            "Epoch 173, training loss: 0.013853\n",
            "Epoch 174, training loss: 0.013851\n",
            "Epoch 175, training loss: 0.013849\n",
            "Epoch 176, training loss: 0.013847\n",
            "Epoch 177, training loss: 0.013845\n",
            "Epoch 178, training loss: 0.013843\n",
            "Epoch 179, training loss: 0.013841\n",
            "Epoch 180, training loss: 0.013839\n",
            "Epoch 181, training loss: 0.013837\n",
            "Epoch 182, training loss: 0.013836\n",
            "Epoch 183, training loss: 0.013834\n",
            "Epoch 184, training loss: 0.013832\n",
            "Epoch 185, training loss: 0.013830\n",
            "Epoch 186, training loss: 0.013828\n",
            "Epoch 187, training loss: 0.013826\n",
            "Epoch 188, training loss: 0.013824\n",
            "Epoch 189, training loss: 0.013822\n",
            "Epoch 190, training loss: 0.013820\n",
            "Epoch 191, training loss: 0.013818\n",
            "Epoch 192, training loss: 0.013816\n",
            "Epoch 193, training loss: 0.013815\n",
            "Epoch 194, training loss: 0.013813\n",
            "Epoch 195, training loss: 0.013811\n",
            "Epoch 196, training loss: 0.013809\n",
            "Epoch 197, training loss: 0.013807\n",
            "Epoch 198, training loss: 0.013806\n",
            "Epoch 199, training loss: 0.013804\n",
            "Epoch 200, training loss: 0.013802\n",
            "Epoch 201, training loss: 0.013800\n",
            "Epoch 202, training loss: 0.013798\n",
            "Epoch 203, training loss: 0.013797\n",
            "Epoch 204, training loss: 0.013795\n",
            "Epoch 205, training loss: 0.013793\n",
            "Epoch 206, training loss: 0.013791\n",
            "Epoch 207, training loss: 0.013790\n",
            "Epoch 208, training loss: 0.013788\n",
            "Epoch 209, training loss: 0.013786\n",
            "Epoch 210, training loss: 0.013784\n",
            "Epoch 211, training loss: 0.013783\n",
            "Epoch 212, training loss: 0.013781\n",
            "Epoch 213, training loss: 0.013779\n",
            "Epoch 214, training loss: 0.013777\n",
            "Epoch 215, training loss: 0.013776\n",
            "Epoch 216, training loss: 0.013774\n",
            "Epoch 217, training loss: 0.013772\n",
            "Epoch 218, training loss: 0.013771\n",
            "Epoch 219, training loss: 0.013769\n",
            "Epoch 220, training loss: 0.013767\n",
            "Epoch 221, training loss: 0.013766\n",
            "Epoch 222, training loss: 0.013764\n",
            "Epoch 223, training loss: 0.013762\n",
            "Epoch 224, training loss: 0.013761\n",
            "Epoch 225, training loss: 0.013759\n",
            "Epoch 226, training loss: 0.013757\n",
            "Epoch 227, training loss: 0.013756\n",
            "Epoch 228, training loss: 0.013754\n",
            "Epoch 229, training loss: 0.013752\n",
            "Epoch 230, training loss: 0.013751\n",
            "Epoch 231, training loss: 0.013749\n",
            "Epoch 232, training loss: 0.013747\n",
            "Epoch 233, training loss: 0.013746\n",
            "Epoch 234, training loss: 0.013744\n",
            "Epoch 235, training loss: 0.013743\n",
            "Epoch 236, training loss: 0.013742\n",
            "Epoch 237, training loss: 0.013740\n",
            "Epoch 238, training loss: 0.013739\n",
            "Epoch 239, training loss: 0.013738\n",
            "Epoch 240, training loss: 0.013736\n",
            "Epoch 241, training loss: 0.013735\n",
            "Epoch 242, training loss: 0.013734\n",
            "Epoch 243, training loss: 0.013732\n",
            "Epoch 244, training loss: 0.013731\n",
            "Epoch 245, training loss: 0.013729\n",
            "Epoch 246, training loss: 0.013728\n",
            "Epoch 247, training loss: 0.013727\n",
            "Epoch 248, training loss: 0.013725\n",
            "Epoch 249, training loss: 0.013724\n",
            "Epoch 250, training loss: 0.013723\n",
            "Epoch 251, training loss: 0.013721\n",
            "Epoch 252, training loss: 0.013720\n",
            "Epoch 253, training loss: 0.013719\n",
            "Epoch 254, training loss: 0.013717\n",
            "Epoch 255, training loss: 0.013716\n",
            "Epoch 256, training loss: 0.013715\n",
            "Epoch 257, training loss: 0.013714\n",
            "Epoch 258, training loss: 0.013712\n",
            "Epoch 259, training loss: 0.013711\n",
            "Epoch 260, training loss: 0.013710\n",
            "Epoch 261, training loss: 0.013709\n",
            "Epoch 262, training loss: 0.013707\n",
            "Epoch 263, training loss: 0.013706\n",
            "Epoch 264, training loss: 0.013705\n",
            "Epoch 265, training loss: 0.013703\n",
            "Epoch 266, training loss: 0.013702\n",
            "Epoch 267, training loss: 0.013701\n",
            "Epoch 268, training loss: 0.013700\n",
            "Epoch 269, training loss: 0.013699\n",
            "Epoch 270, training loss: 0.013697\n",
            "Epoch 271, training loss: 0.013696\n",
            "Epoch 272, training loss: 0.013695\n",
            "Epoch 273, training loss: 0.013694\n",
            "Epoch 274, training loss: 0.013692\n",
            "Epoch 275, training loss: 0.013691\n",
            "Epoch 276, training loss: 0.013690\n",
            "Epoch 277, training loss: 0.013689\n",
            "Epoch 278, training loss: 0.013688\n",
            "Epoch 279, training loss: 0.013686\n",
            "Epoch 280, training loss: 0.013685\n",
            "Epoch 281, training loss: 0.013684\n",
            "Epoch 282, training loss: 0.013683\n",
            "Epoch 283, training loss: 0.013682\n",
            "Epoch 284, training loss: 0.013680\n",
            "Epoch 285, training loss: 0.013679\n",
            "Epoch 286, training loss: 0.013678\n",
            "Epoch 287, training loss: 0.013677\n",
            "Epoch 288, training loss: 0.013676\n",
            "Epoch 289, training loss: 0.013675\n",
            "Epoch 290, training loss: 0.013673\n",
            "Epoch 291, training loss: 0.013672\n",
            "Epoch 292, training loss: 0.013671\n",
            "Epoch 293, training loss: 0.013670\n",
            "Epoch 294, training loss: 0.013669\n",
            "Epoch 295, training loss: 0.013668\n",
            "Epoch 296, training loss: 0.013666\n",
            "Epoch 297, training loss: 0.013665\n",
            "Epoch 298, training loss: 0.013664\n",
            "Epoch 299, training loss: 0.013663\n",
            "Epoch 300, training loss: 0.013662\n",
            "Epoch 301, training loss: 0.013661\n",
            "Epoch 302, training loss: 0.013660\n",
            "Epoch 303, training loss: 0.013659\n",
            "Epoch 304, training loss: 0.013658\n",
            "Epoch 305, training loss: 0.013656\n",
            "Epoch 306, training loss: 0.013655\n",
            "Epoch 307, training loss: 0.013654\n",
            "Epoch 308, training loss: 0.013653\n",
            "Epoch 309, training loss: 0.013652\n",
            "Epoch 310, training loss: 0.013651\n",
            "Epoch 311, training loss: 0.013650\n",
            "Epoch 312, training loss: 0.013649\n",
            "Epoch 313, training loss: 0.013648\n",
            "Epoch 314, training loss: 0.013647\n",
            "Epoch 315, training loss: 0.013646\n",
            "Epoch 316, training loss: 0.013644\n",
            "Epoch 317, training loss: 0.013644\n",
            "Epoch 318, training loss: 0.013642\n",
            "Epoch 319, training loss: 0.013641\n",
            "Epoch 320, training loss: 0.013640\n",
            "Epoch 321, training loss: 0.013639\n",
            "Epoch 322, training loss: 0.013638\n",
            "Epoch 323, training loss: 0.013637\n",
            "Epoch 324, training loss: 0.013636\n",
            "Epoch 325, training loss: 0.013635\n",
            "Epoch 326, training loss: 0.013634\n",
            "Epoch 327, training loss: 0.013633\n",
            "Epoch 328, training loss: 0.013632\n",
            "Epoch 329, training loss: 0.013631\n",
            "Epoch 330, training loss: 0.013630\n",
            "Epoch 331, training loss: 0.013629\n",
            "Epoch 332, training loss: 0.013628\n",
            "Epoch 333, training loss: 0.013627\n",
            "Epoch 334, training loss: 0.013626\n",
            "Epoch 335, training loss: 0.013625\n",
            "Epoch 336, training loss: 0.013624\n",
            "Epoch 337, training loss: 0.013623\n",
            "Epoch 338, training loss: 0.013622\n",
            "Epoch 339, training loss: 0.013621\n",
            "Epoch 340, training loss: 0.013620\n",
            "Epoch 341, training loss: 0.013619\n",
            "Epoch 342, training loss: 0.013618\n",
            "Epoch 343, training loss: 0.013617\n",
            "Epoch 344, training loss: 0.013616\n",
            "Epoch 345, training loss: 0.013615\n",
            "Epoch 346, training loss: 0.013614\n",
            "Epoch 347, training loss: 0.013613\n",
            "Epoch 348, training loss: 0.013612\n",
            "Epoch 349, training loss: 0.013611\n",
            "Epoch 350, training loss: 0.013610\n",
            "Epoch 351, training loss: 0.013609\n",
            "Epoch 352, training loss: 0.013608\n",
            "Epoch 353, training loss: 0.013607\n",
            "Epoch 354, training loss: 0.013606\n",
            "Epoch 355, training loss: 0.013605\n",
            "Epoch 356, training loss: 0.013604\n",
            "Epoch 357, training loss: 0.013603\n",
            "Epoch 358, training loss: 0.013602\n",
            "Epoch 359, training loss: 0.013601\n",
            "Epoch 360, training loss: 0.013600\n",
            "Epoch 361, training loss: 0.013599\n",
            "Epoch 362, training loss: 0.013598\n",
            "Epoch 363, training loss: 0.013598\n",
            "Epoch 364, training loss: 0.013597\n",
            "Epoch 365, training loss: 0.013596\n",
            "Epoch 366, training loss: 0.013595\n",
            "Epoch 367, training loss: 0.013594\n",
            "Epoch 368, training loss: 0.013593\n",
            "Epoch 369, training loss: 0.013592\n",
            "Epoch 370, training loss: 0.013591\n",
            "Epoch 371, training loss: 0.013590\n",
            "Epoch 372, training loss: 0.013589\n",
            "Epoch 373, training loss: 0.013588\n",
            "Epoch 374, training loss: 0.013587\n",
            "Epoch 375, training loss: 0.013586\n",
            "Epoch 376, training loss: 0.013586\n",
            "Epoch 377, training loss: 0.013585\n",
            "Epoch 378, training loss: 0.013584\n",
            "Epoch 379, training loss: 0.013583\n",
            "Epoch 380, training loss: 0.013582\n",
            "Epoch 381, training loss: 0.013581\n",
            "Epoch 382, training loss: 0.013580\n",
            "Epoch 383, training loss: 0.013579\n",
            "Epoch 384, training loss: 0.013578\n",
            "Epoch 385, training loss: 0.013577\n",
            "Epoch 386, training loss: 0.013576\n",
            "Epoch 387, training loss: 0.013576\n",
            "Epoch 388, training loss: 0.013575\n",
            "Epoch 389, training loss: 0.013574\n",
            "Epoch 390, training loss: 0.013573\n",
            "Epoch 391, training loss: 0.013572\n",
            "Epoch 392, training loss: 0.013571\n",
            "Epoch 393, training loss: 0.013570\n",
            "Epoch 394, training loss: 0.013569\n",
            "Epoch 395, training loss: 0.013569\n",
            "Epoch 396, training loss: 0.013568\n",
            "Epoch 397, training loss: 0.013567\n",
            "Epoch 398, training loss: 0.013566\n",
            "Epoch 399, training loss: 0.013565\n",
            "Epoch 400, training loss: 0.013564\n",
            "Epoch 401, training loss: 0.013564\n",
            "Epoch 402, training loss: 0.013563\n",
            "Epoch 403, training loss: 0.013562\n",
            "Epoch 404, training loss: 0.013561\n",
            "Epoch 405, training loss: 0.013560\n",
            "Epoch 406, training loss: 0.013559\n",
            "Epoch 407, training loss: 0.013558\n",
            "Epoch 408, training loss: 0.013558\n",
            "Epoch 409, training loss: 0.013557\n",
            "Epoch 410, training loss: 0.013556\n",
            "Epoch 411, training loss: 0.013555\n",
            "Epoch 412, training loss: 0.013554\n",
            "Epoch 413, training loss: 0.013553\n",
            "Epoch 414, training loss: 0.013553\n",
            "Epoch 415, training loss: 0.013552\n",
            "Epoch 416, training loss: 0.013551\n",
            "Epoch 417, training loss: 0.013550\n",
            "Epoch 418, training loss: 0.013549\n",
            "Epoch 419, training loss: 0.013549\n",
            "Epoch 420, training loss: 0.013548\n",
            "Epoch 421, training loss: 0.013547\n",
            "Epoch 422, training loss: 0.013546\n",
            "Epoch 423, training loss: 0.013545\n",
            "Epoch 424, training loss: 0.013544\n",
            "Epoch 425, training loss: 0.013544\n",
            "Epoch 426, training loss: 0.013543\n",
            "Epoch 427, training loss: 0.013542\n",
            "Epoch 428, training loss: 0.013541\n",
            "Epoch 429, training loss: 0.013540\n",
            "Epoch 430, training loss: 0.013540\n",
            "Epoch 431, training loss: 0.013539\n",
            "Epoch 432, training loss: 0.013538\n",
            "Epoch 433, training loss: 0.013537\n",
            "Epoch 434, training loss: 0.013536\n",
            "Epoch 435, training loss: 0.013536\n",
            "Epoch 436, training loss: 0.013535\n",
            "Epoch 437, training loss: 0.013534\n",
            "Epoch 438, training loss: 0.013533\n",
            "Epoch 439, training loss: 0.013533\n",
            "Epoch 440, training loss: 0.013532\n",
            "Epoch 441, training loss: 0.013531\n",
            "Epoch 442, training loss: 0.013530\n",
            "Epoch 443, training loss: 0.013529\n",
            "Epoch 444, training loss: 0.013529\n",
            "Epoch 445, training loss: 0.013528\n",
            "Epoch 446, training loss: 0.013527\n",
            "Epoch 447, training loss: 0.013526\n",
            "Epoch 448, training loss: 0.013526\n",
            "Epoch 449, training loss: 0.013525\n",
            "Epoch 450, training loss: 0.013524\n",
            "Epoch 451, training loss: 0.013523\n",
            "Epoch 452, training loss: 0.013522\n",
            "Epoch 453, training loss: 0.013522\n",
            "Epoch 454, training loss: 0.013521\n",
            "Epoch 455, training loss: 0.013520\n",
            "Epoch 456, training loss: 0.013519\n",
            "Epoch 457, training loss: 0.013519\n",
            "Epoch 458, training loss: 0.013518\n",
            "Epoch 459, training loss: 0.013517\n",
            "Epoch 460, training loss: 0.013516\n",
            "Epoch 461, training loss: 0.013516\n",
            "Epoch 462, training loss: 0.013515\n",
            "Epoch 463, training loss: 0.013514\n",
            "Epoch 464, training loss: 0.013513\n",
            "Epoch 465, training loss: 0.013513\n",
            "Epoch 466, training loss: 0.013512\n",
            "Epoch 467, training loss: 0.013511\n",
            "Epoch 468, training loss: 0.013511\n",
            "Epoch 469, training loss: 0.013510\n",
            "Epoch 470, training loss: 0.013509\n",
            "Epoch 471, training loss: 0.013508\n",
            "Epoch 472, training loss: 0.013507\n",
            "Epoch 473, training loss: 0.013507\n",
            "Epoch 474, training loss: 0.013506\n",
            "Epoch 475, training loss: 0.013505\n",
            "Epoch 476, training loss: 0.013505\n",
            "Epoch 477, training loss: 0.013504\n",
            "Epoch 478, training loss: 0.013503\n",
            "Epoch 479, training loss: 0.013502\n",
            "Epoch 480, training loss: 0.013502\n",
            "Epoch 481, training loss: 0.013501\n",
            "Epoch 482, training loss: 0.013500\n",
            "Epoch 483, training loss: 0.013500\n",
            "Epoch 484, training loss: 0.013499\n",
            "Epoch 485, training loss: 0.013498\n",
            "Epoch 486, training loss: 0.013498\n",
            "Epoch 487, training loss: 0.013497\n",
            "Epoch 488, training loss: 0.013496\n",
            "Epoch 489, training loss: 0.013495\n",
            "Epoch 490, training loss: 0.013495\n",
            "Epoch 491, training loss: 0.013494\n",
            "Epoch 492, training loss: 0.013493\n",
            "Epoch 493, training loss: 0.013492\n",
            "Epoch 494, training loss: 0.013492\n",
            "Epoch 495, training loss: 0.013491\n",
            "Epoch 496, training loss: 0.013490\n",
            "Epoch 497, training loss: 0.013490\n",
            "Epoch 498, training loss: 0.013489\n",
            "Epoch 499, training loss: 0.013488\n",
            "Epoch 0, testing loss: 0.021905\n",
            "Epoch 1, testing loss: 0.021905\n",
            "Epoch 2, testing loss: 0.021905\n",
            "Epoch 3, testing loss: 0.021905\n",
            "Epoch 4, testing loss: 0.021905\n",
            "Epoch 5, testing loss: 0.021905\n",
            "Epoch 6, testing loss: 0.021905\n",
            "Epoch 7, testing loss: 0.021905\n",
            "Epoch 8, testing loss: 0.021905\n",
            "Epoch 9, testing loss: 0.021905\n",
            "Epoch 10, testing loss: 0.021905\n",
            "Epoch 11, testing loss: 0.021905\n",
            "Epoch 12, testing loss: 0.021905\n",
            "Epoch 13, testing loss: 0.021905\n",
            "Epoch 14, testing loss: 0.021905\n",
            "Epoch 15, testing loss: 0.021905\n",
            "Epoch 16, testing loss: 0.021905\n",
            "Epoch 17, testing loss: 0.021905\n",
            "Epoch 18, testing loss: 0.021905\n",
            "Epoch 19, testing loss: 0.021905\n",
            "Epoch 20, testing loss: 0.021905\n",
            "Epoch 21, testing loss: 0.021905\n",
            "Epoch 22, testing loss: 0.021905\n",
            "Epoch 23, testing loss: 0.021905\n",
            "Epoch 24, testing loss: 0.021905\n",
            "Epoch 25, testing loss: 0.021905\n",
            "Epoch 26, testing loss: 0.021905\n",
            "Epoch 27, testing loss: 0.021905\n",
            "Epoch 28, testing loss: 0.021905\n",
            "Epoch 29, testing loss: 0.021905\n",
            "Epoch 30, testing loss: 0.021905\n",
            "Epoch 31, testing loss: 0.021905\n",
            "Epoch 32, testing loss: 0.021905\n",
            "Epoch 33, testing loss: 0.021905\n",
            "Epoch 34, testing loss: 0.021905\n",
            "Epoch 35, testing loss: 0.021905\n",
            "Epoch 36, testing loss: 0.021905\n",
            "Epoch 37, testing loss: 0.021905\n",
            "Epoch 38, testing loss: 0.021905\n",
            "Epoch 39, testing loss: 0.021905\n",
            "Epoch 40, testing loss: 0.021905\n",
            "Epoch 41, testing loss: 0.021905\n",
            "Epoch 42, testing loss: 0.021905\n",
            "Epoch 43, testing loss: 0.021905\n",
            "Epoch 44, testing loss: 0.021905\n",
            "Epoch 45, testing loss: 0.021905\n",
            "Epoch 46, testing loss: 0.021905\n",
            "Epoch 47, testing loss: 0.021905\n",
            "Epoch 48, testing loss: 0.021905\n",
            "Epoch 49, testing loss: 0.021905\n",
            "Epoch 50, testing loss: 0.021905\n",
            "Epoch 51, testing loss: 0.021905\n",
            "Epoch 52, testing loss: 0.021905\n",
            "Epoch 53, testing loss: 0.021905\n",
            "Epoch 54, testing loss: 0.021905\n",
            "Epoch 55, testing loss: 0.021905\n",
            "Epoch 56, testing loss: 0.021905\n",
            "Epoch 57, testing loss: 0.021905\n",
            "Epoch 58, testing loss: 0.021905\n",
            "Epoch 59, testing loss: 0.021905\n",
            "Epoch 60, testing loss: 0.021905\n",
            "Epoch 61, testing loss: 0.021905\n",
            "Epoch 62, testing loss: 0.021905\n",
            "Epoch 63, testing loss: 0.021905\n",
            "Epoch 64, testing loss: 0.021905\n",
            "Epoch 65, testing loss: 0.021905\n",
            "Epoch 66, testing loss: 0.021905\n",
            "Epoch 67, testing loss: 0.021905\n",
            "Epoch 68, testing loss: 0.021905\n",
            "Epoch 69, testing loss: 0.021905\n",
            "Epoch 70, testing loss: 0.021905\n",
            "Epoch 71, testing loss: 0.021905\n",
            "Epoch 72, testing loss: 0.021905\n",
            "Epoch 73, testing loss: 0.021905\n",
            "Epoch 74, testing loss: 0.021905\n",
            "Epoch 75, testing loss: 0.021905\n",
            "Epoch 76, testing loss: 0.021905\n",
            "Epoch 77, testing loss: 0.021905\n",
            "Epoch 78, testing loss: 0.021905\n",
            "Epoch 79, testing loss: 0.021905\n",
            "Epoch 80, testing loss: 0.021905\n",
            "Epoch 81, testing loss: 0.021905\n",
            "Epoch 82, testing loss: 0.021905\n",
            "Epoch 83, testing loss: 0.021905\n",
            "Epoch 84, testing loss: 0.021905\n",
            "Epoch 85, testing loss: 0.021905\n",
            "Epoch 86, testing loss: 0.021905\n",
            "Epoch 87, testing loss: 0.021905\n",
            "Epoch 88, testing loss: 0.021905\n",
            "Epoch 89, testing loss: 0.021905\n",
            "Epoch 90, testing loss: 0.021905\n",
            "Epoch 91, testing loss: 0.021905\n",
            "Epoch 92, testing loss: 0.021905\n",
            "Epoch 93, testing loss: 0.021905\n",
            "Epoch 94, testing loss: 0.021905\n",
            "Epoch 95, testing loss: 0.021905\n",
            "Epoch 96, testing loss: 0.021905\n",
            "Epoch 97, testing loss: 0.021905\n",
            "Epoch 98, testing loss: 0.021905\n",
            "Epoch 99, testing loss: 0.021905\n",
            "Epoch 100, testing loss: 0.021905\n",
            "Epoch 101, testing loss: 0.021905\n",
            "Epoch 102, testing loss: 0.021905\n",
            "Epoch 103, testing loss: 0.021905\n",
            "Epoch 104, testing loss: 0.021905\n",
            "Epoch 105, testing loss: 0.021905\n",
            "Epoch 106, testing loss: 0.021905\n",
            "Epoch 107, testing loss: 0.021905\n",
            "Epoch 108, testing loss: 0.021905\n",
            "Epoch 109, testing loss: 0.021905\n",
            "Epoch 110, testing loss: 0.021905\n",
            "Epoch 111, testing loss: 0.021905\n",
            "Epoch 112, testing loss: 0.021905\n",
            "Epoch 113, testing loss: 0.021905\n",
            "Epoch 114, testing loss: 0.021905\n",
            "Epoch 115, testing loss: 0.021905\n",
            "Epoch 116, testing loss: 0.021905\n",
            "Epoch 117, testing loss: 0.021905\n",
            "Epoch 118, testing loss: 0.021905\n",
            "Epoch 119, testing loss: 0.021905\n",
            "Epoch 120, testing loss: 0.021905\n",
            "Epoch 121, testing loss: 0.021905\n",
            "Epoch 122, testing loss: 0.021905\n",
            "Epoch 123, testing loss: 0.021905\n",
            "Epoch 124, testing loss: 0.021905\n",
            "Epoch 125, testing loss: 0.021905\n",
            "Epoch 126, testing loss: 0.021905\n",
            "Epoch 127, testing loss: 0.021905\n",
            "Epoch 128, testing loss: 0.021905\n",
            "Epoch 129, testing loss: 0.021905\n",
            "Epoch 130, testing loss: 0.021905\n",
            "Epoch 131, testing loss: 0.021905\n",
            "Epoch 132, testing loss: 0.021905\n",
            "Epoch 133, testing loss: 0.021905\n",
            "Epoch 134, testing loss: 0.021905\n",
            "Epoch 135, testing loss: 0.021905\n",
            "Epoch 136, testing loss: 0.021905\n",
            "Epoch 137, testing loss: 0.021905\n",
            "Epoch 138, testing loss: 0.021905\n",
            "Epoch 139, testing loss: 0.021905\n",
            "Epoch 140, testing loss: 0.021905\n",
            "Epoch 141, testing loss: 0.021905\n",
            "Epoch 142, testing loss: 0.021905\n",
            "Epoch 143, testing loss: 0.021905\n",
            "Epoch 144, testing loss: 0.021905\n",
            "Epoch 145, testing loss: 0.021905\n",
            "Epoch 146, testing loss: 0.021905\n",
            "Epoch 147, testing loss: 0.021905\n",
            "Epoch 148, testing loss: 0.021905\n",
            "Epoch 149, testing loss: 0.021905\n",
            "Epoch 150, testing loss: 0.021905\n",
            "Epoch 151, testing loss: 0.021905\n",
            "Epoch 152, testing loss: 0.021905\n",
            "Epoch 153, testing loss: 0.021905\n",
            "Epoch 154, testing loss: 0.021905\n",
            "Epoch 155, testing loss: 0.021905\n",
            "Epoch 156, testing loss: 0.021905\n",
            "Epoch 157, testing loss: 0.021905\n",
            "Epoch 158, testing loss: 0.021905\n",
            "Epoch 159, testing loss: 0.021905\n",
            "Epoch 160, testing loss: 0.021905\n",
            "Epoch 161, testing loss: 0.021905\n",
            "Epoch 162, testing loss: 0.021905\n",
            "Epoch 163, testing loss: 0.021905\n",
            "Epoch 164, testing loss: 0.021905\n",
            "Epoch 165, testing loss: 0.021905\n",
            "Epoch 166, testing loss: 0.021905\n",
            "Epoch 167, testing loss: 0.021905\n",
            "Epoch 168, testing loss: 0.021905\n",
            "Epoch 169, testing loss: 0.021905\n",
            "Epoch 170, testing loss: 0.021905\n",
            "Epoch 171, testing loss: 0.021905\n",
            "Epoch 172, testing loss: 0.021905\n",
            "Epoch 173, testing loss: 0.021905\n",
            "Epoch 174, testing loss: 0.021905\n",
            "Epoch 175, testing loss: 0.021905\n",
            "Epoch 176, testing loss: 0.021905\n",
            "Epoch 177, testing loss: 0.021905\n",
            "Epoch 178, testing loss: 0.021905\n",
            "Epoch 179, testing loss: 0.021905\n",
            "Epoch 180, testing loss: 0.021905\n",
            "Epoch 181, testing loss: 0.021905\n",
            "Epoch 182, testing loss: 0.021905\n",
            "Epoch 183, testing loss: 0.021905\n",
            "Epoch 184, testing loss: 0.021905\n",
            "Epoch 185, testing loss: 0.021905\n",
            "Epoch 186, testing loss: 0.021905\n",
            "Epoch 187, testing loss: 0.021905\n",
            "Epoch 188, testing loss: 0.021905\n",
            "Epoch 189, testing loss: 0.021905\n",
            "Epoch 190, testing loss: 0.021905\n",
            "Epoch 191, testing loss: 0.021905\n",
            "Epoch 192, testing loss: 0.021905\n",
            "Epoch 193, testing loss: 0.021905\n",
            "Epoch 194, testing loss: 0.021905\n",
            "Epoch 195, testing loss: 0.021905\n",
            "Epoch 196, testing loss: 0.021905\n",
            "Epoch 197, testing loss: 0.021905\n",
            "Epoch 198, testing loss: 0.021905\n",
            "Epoch 199, testing loss: 0.021905\n",
            "Epoch 200, testing loss: 0.021905\n",
            "Epoch 201, testing loss: 0.021905\n",
            "Epoch 202, testing loss: 0.021905\n",
            "Epoch 203, testing loss: 0.021905\n",
            "Epoch 204, testing loss: 0.021905\n",
            "Epoch 205, testing loss: 0.021905\n",
            "Epoch 206, testing loss: 0.021905\n",
            "Epoch 207, testing loss: 0.021905\n",
            "Epoch 208, testing loss: 0.021905\n",
            "Epoch 209, testing loss: 0.021905\n",
            "Epoch 210, testing loss: 0.021905\n",
            "Epoch 211, testing loss: 0.021905\n",
            "Epoch 212, testing loss: 0.021905\n",
            "Epoch 213, testing loss: 0.021905\n",
            "Epoch 214, testing loss: 0.021905\n",
            "Epoch 215, testing loss: 0.021905\n",
            "Epoch 216, testing loss: 0.021905\n",
            "Epoch 217, testing loss: 0.021905\n",
            "Epoch 218, testing loss: 0.021905\n",
            "Epoch 219, testing loss: 0.021905\n",
            "Epoch 220, testing loss: 0.021905\n",
            "Epoch 221, testing loss: 0.021905\n",
            "Epoch 222, testing loss: 0.021905\n",
            "Epoch 223, testing loss: 0.021905\n",
            "Epoch 224, testing loss: 0.021905\n",
            "Epoch 225, testing loss: 0.021905\n",
            "Epoch 226, testing loss: 0.021905\n",
            "Epoch 227, testing loss: 0.021905\n",
            "Epoch 228, testing loss: 0.021905\n",
            "Epoch 229, testing loss: 0.021905\n",
            "Epoch 230, testing loss: 0.021905\n",
            "Epoch 231, testing loss: 0.021905\n",
            "Epoch 232, testing loss: 0.021905\n",
            "Epoch 233, testing loss: 0.021905\n",
            "Epoch 234, testing loss: 0.021905\n",
            "Epoch 235, testing loss: 0.021905\n",
            "Epoch 236, testing loss: 0.021905\n",
            "Epoch 237, testing loss: 0.021905\n",
            "Epoch 238, testing loss: 0.021905\n",
            "Epoch 239, testing loss: 0.021905\n",
            "Epoch 240, testing loss: 0.021905\n",
            "Epoch 241, testing loss: 0.021905\n",
            "Epoch 242, testing loss: 0.021905\n",
            "Epoch 243, testing loss: 0.021905\n",
            "Epoch 244, testing loss: 0.021905\n",
            "Epoch 245, testing loss: 0.021905\n",
            "Epoch 246, testing loss: 0.021905\n",
            "Epoch 247, testing loss: 0.021905\n",
            "Epoch 248, testing loss: 0.021905\n",
            "Epoch 249, testing loss: 0.021905\n",
            "Epoch 250, testing loss: 0.021905\n",
            "Epoch 251, testing loss: 0.021905\n",
            "Epoch 252, testing loss: 0.021905\n",
            "Epoch 253, testing loss: 0.021905\n",
            "Epoch 254, testing loss: 0.021905\n",
            "Epoch 255, testing loss: 0.021905\n",
            "Epoch 256, testing loss: 0.021905\n",
            "Epoch 257, testing loss: 0.021905\n",
            "Epoch 258, testing loss: 0.021905\n",
            "Epoch 259, testing loss: 0.021905\n",
            "Epoch 260, testing loss: 0.021905\n",
            "Epoch 261, testing loss: 0.021905\n",
            "Epoch 262, testing loss: 0.021905\n",
            "Epoch 263, testing loss: 0.021905\n",
            "Epoch 264, testing loss: 0.021905\n",
            "Epoch 265, testing loss: 0.021905\n",
            "Epoch 266, testing loss: 0.021905\n",
            "Epoch 267, testing loss: 0.021905\n",
            "Epoch 268, testing loss: 0.021905\n",
            "Epoch 269, testing loss: 0.021905\n",
            "Epoch 270, testing loss: 0.021905\n",
            "Epoch 271, testing loss: 0.021905\n",
            "Epoch 272, testing loss: 0.021905\n",
            "Epoch 273, testing loss: 0.021905\n",
            "Epoch 274, testing loss: 0.021905\n",
            "Epoch 275, testing loss: 0.021905\n",
            "Epoch 276, testing loss: 0.021905\n",
            "Epoch 277, testing loss: 0.021905\n",
            "Epoch 278, testing loss: 0.021905\n",
            "Epoch 279, testing loss: 0.021905\n",
            "Epoch 280, testing loss: 0.021905\n",
            "Epoch 281, testing loss: 0.021905\n",
            "Epoch 282, testing loss: 0.021905\n",
            "Epoch 283, testing loss: 0.021905\n",
            "Epoch 284, testing loss: 0.021905\n",
            "Epoch 285, testing loss: 0.021905\n",
            "Epoch 286, testing loss: 0.021905\n",
            "Epoch 287, testing loss: 0.021905\n",
            "Epoch 288, testing loss: 0.021905\n",
            "Epoch 289, testing loss: 0.021905\n",
            "Epoch 290, testing loss: 0.021905\n",
            "Epoch 291, testing loss: 0.021905\n",
            "Epoch 292, testing loss: 0.021905\n",
            "Epoch 293, testing loss: 0.021905\n",
            "Epoch 294, testing loss: 0.021905\n",
            "Epoch 295, testing loss: 0.021905\n",
            "Epoch 296, testing loss: 0.021905\n",
            "Epoch 297, testing loss: 0.021905\n",
            "Epoch 298, testing loss: 0.021905\n",
            "Epoch 299, testing loss: 0.021905\n",
            "Epoch 300, testing loss: 0.021905\n",
            "Epoch 301, testing loss: 0.021905\n",
            "Epoch 302, testing loss: 0.021905\n",
            "Epoch 303, testing loss: 0.021905\n",
            "Epoch 304, testing loss: 0.021905\n",
            "Epoch 305, testing loss: 0.021905\n",
            "Epoch 306, testing loss: 0.021905\n",
            "Epoch 307, testing loss: 0.021905\n",
            "Epoch 308, testing loss: 0.021905\n",
            "Epoch 309, testing loss: 0.021905\n",
            "Epoch 310, testing loss: 0.021905\n",
            "Epoch 311, testing loss: 0.021905\n",
            "Epoch 312, testing loss: 0.021905\n",
            "Epoch 313, testing loss: 0.021905\n",
            "Epoch 314, testing loss: 0.021905\n",
            "Epoch 315, testing loss: 0.021905\n",
            "Epoch 316, testing loss: 0.021905\n",
            "Epoch 317, testing loss: 0.021905\n",
            "Epoch 318, testing loss: 0.021905\n",
            "Epoch 319, testing loss: 0.021905\n",
            "Epoch 320, testing loss: 0.021905\n",
            "Epoch 321, testing loss: 0.021905\n",
            "Epoch 322, testing loss: 0.021905\n",
            "Epoch 323, testing loss: 0.021905\n",
            "Epoch 324, testing loss: 0.021905\n",
            "Epoch 325, testing loss: 0.021905\n",
            "Epoch 326, testing loss: 0.021905\n",
            "Epoch 327, testing loss: 0.021905\n",
            "Epoch 328, testing loss: 0.021905\n",
            "Epoch 329, testing loss: 0.021905\n",
            "Epoch 330, testing loss: 0.021905\n",
            "Epoch 331, testing loss: 0.021905\n",
            "Epoch 332, testing loss: 0.021905\n",
            "Epoch 333, testing loss: 0.021905\n",
            "Epoch 334, testing loss: 0.021905\n",
            "Epoch 335, testing loss: 0.021905\n",
            "Epoch 336, testing loss: 0.021905\n",
            "Epoch 337, testing loss: 0.021905\n",
            "Epoch 338, testing loss: 0.021905\n",
            "Epoch 339, testing loss: 0.021905\n",
            "Epoch 340, testing loss: 0.021905\n",
            "Epoch 341, testing loss: 0.021905\n",
            "Epoch 342, testing loss: 0.021905\n",
            "Epoch 343, testing loss: 0.021905\n",
            "Epoch 344, testing loss: 0.021905\n",
            "Epoch 345, testing loss: 0.021905\n",
            "Epoch 346, testing loss: 0.021905\n",
            "Epoch 347, testing loss: 0.021905\n",
            "Epoch 348, testing loss: 0.021905\n",
            "Epoch 349, testing loss: 0.021905\n",
            "Epoch 350, testing loss: 0.021905\n",
            "Epoch 351, testing loss: 0.021905\n",
            "Epoch 352, testing loss: 0.021905\n",
            "Epoch 353, testing loss: 0.021905\n",
            "Epoch 354, testing loss: 0.021905\n",
            "Epoch 355, testing loss: 0.021905\n",
            "Epoch 356, testing loss: 0.021905\n",
            "Epoch 357, testing loss: 0.021905\n",
            "Epoch 358, testing loss: 0.021905\n",
            "Epoch 359, testing loss: 0.021905\n",
            "Epoch 360, testing loss: 0.021905\n",
            "Epoch 361, testing loss: 0.021905\n",
            "Epoch 362, testing loss: 0.021905\n",
            "Epoch 363, testing loss: 0.021905\n",
            "Epoch 364, testing loss: 0.021905\n",
            "Epoch 365, testing loss: 0.021905\n",
            "Epoch 366, testing loss: 0.021905\n",
            "Epoch 367, testing loss: 0.021905\n",
            "Epoch 368, testing loss: 0.021905\n",
            "Epoch 369, testing loss: 0.021905\n",
            "Epoch 370, testing loss: 0.021905\n",
            "Epoch 371, testing loss: 0.021905\n",
            "Epoch 372, testing loss: 0.021905\n",
            "Epoch 373, testing loss: 0.021905\n",
            "Epoch 374, testing loss: 0.021905\n",
            "Epoch 375, testing loss: 0.021905\n",
            "Epoch 376, testing loss: 0.021905\n",
            "Epoch 377, testing loss: 0.021905\n",
            "Epoch 378, testing loss: 0.021905\n",
            "Epoch 379, testing loss: 0.021905\n",
            "Epoch 380, testing loss: 0.021905\n",
            "Epoch 381, testing loss: 0.021905\n",
            "Epoch 382, testing loss: 0.021905\n",
            "Epoch 383, testing loss: 0.021905\n",
            "Epoch 384, testing loss: 0.021905\n",
            "Epoch 385, testing loss: 0.021905\n",
            "Epoch 386, testing loss: 0.021905\n",
            "Epoch 387, testing loss: 0.021905\n",
            "Epoch 388, testing loss: 0.021905\n",
            "Epoch 389, testing loss: 0.021905\n",
            "Epoch 390, testing loss: 0.021905\n",
            "Epoch 391, testing loss: 0.021905\n",
            "Epoch 392, testing loss: 0.021905\n",
            "Epoch 393, testing loss: 0.021905\n",
            "Epoch 394, testing loss: 0.021905\n",
            "Epoch 395, testing loss: 0.021905\n",
            "Epoch 396, testing loss: 0.021905\n",
            "Epoch 397, testing loss: 0.021905\n",
            "Epoch 398, testing loss: 0.021905\n",
            "Epoch 399, testing loss: 0.021905\n",
            "Epoch 400, testing loss: 0.021905\n",
            "Epoch 401, testing loss: 0.021905\n",
            "Epoch 402, testing loss: 0.021905\n",
            "Epoch 403, testing loss: 0.021905\n",
            "Epoch 404, testing loss: 0.021905\n",
            "Epoch 405, testing loss: 0.021905\n",
            "Epoch 406, testing loss: 0.021905\n",
            "Epoch 407, testing loss: 0.021905\n",
            "Epoch 408, testing loss: 0.021905\n",
            "Epoch 409, testing loss: 0.021905\n",
            "Epoch 410, testing loss: 0.021905\n",
            "Epoch 411, testing loss: 0.021905\n",
            "Epoch 412, testing loss: 0.021905\n",
            "Epoch 413, testing loss: 0.021905\n",
            "Epoch 414, testing loss: 0.021905\n",
            "Epoch 415, testing loss: 0.021905\n",
            "Epoch 416, testing loss: 0.021905\n",
            "Epoch 417, testing loss: 0.021905\n",
            "Epoch 418, testing loss: 0.021905\n",
            "Epoch 419, testing loss: 0.021905\n",
            "Epoch 420, testing loss: 0.021905\n",
            "Epoch 421, testing loss: 0.021905\n",
            "Epoch 422, testing loss: 0.021905\n",
            "Epoch 423, testing loss: 0.021905\n",
            "Epoch 424, testing loss: 0.021905\n",
            "Epoch 425, testing loss: 0.021905\n",
            "Epoch 426, testing loss: 0.021905\n",
            "Epoch 427, testing loss: 0.021905\n",
            "Epoch 428, testing loss: 0.021905\n",
            "Epoch 429, testing loss: 0.021905\n",
            "Epoch 430, testing loss: 0.021905\n",
            "Epoch 431, testing loss: 0.021905\n",
            "Epoch 432, testing loss: 0.021905\n",
            "Epoch 433, testing loss: 0.021905\n",
            "Epoch 434, testing loss: 0.021905\n",
            "Epoch 435, testing loss: 0.021905\n",
            "Epoch 436, testing loss: 0.021905\n",
            "Epoch 437, testing loss: 0.021905\n",
            "Epoch 438, testing loss: 0.021905\n",
            "Epoch 439, testing loss: 0.021905\n",
            "Epoch 440, testing loss: 0.021905\n",
            "Epoch 441, testing loss: 0.021905\n",
            "Epoch 442, testing loss: 0.021905\n",
            "Epoch 443, testing loss: 0.021905\n",
            "Epoch 444, testing loss: 0.021905\n",
            "Epoch 445, testing loss: 0.021905\n",
            "Epoch 446, testing loss: 0.021905\n",
            "Epoch 447, testing loss: 0.021905\n",
            "Epoch 448, testing loss: 0.021905\n",
            "Epoch 449, testing loss: 0.021905\n",
            "Epoch 450, testing loss: 0.021905\n",
            "Epoch 451, testing loss: 0.021905\n",
            "Epoch 452, testing loss: 0.021905\n",
            "Epoch 453, testing loss: 0.021905\n",
            "Epoch 454, testing loss: 0.021905\n",
            "Epoch 455, testing loss: 0.021905\n",
            "Epoch 456, testing loss: 0.021905\n",
            "Epoch 457, testing loss: 0.021905\n",
            "Epoch 458, testing loss: 0.021905\n",
            "Epoch 459, testing loss: 0.021905\n",
            "Epoch 460, testing loss: 0.021905\n",
            "Epoch 461, testing loss: 0.021905\n",
            "Epoch 462, testing loss: 0.021905\n",
            "Epoch 463, testing loss: 0.021905\n",
            "Epoch 464, testing loss: 0.021905\n",
            "Epoch 465, testing loss: 0.021905\n",
            "Epoch 466, testing loss: 0.021905\n",
            "Epoch 467, testing loss: 0.021905\n",
            "Epoch 468, testing loss: 0.021905\n",
            "Epoch 469, testing loss: 0.021905\n",
            "Epoch 470, testing loss: 0.021905\n",
            "Epoch 471, testing loss: 0.021905\n",
            "Epoch 472, testing loss: 0.021905\n",
            "Epoch 473, testing loss: 0.021905\n",
            "Epoch 474, testing loss: 0.021905\n",
            "Epoch 475, testing loss: 0.021905\n",
            "Epoch 476, testing loss: 0.021905\n",
            "Epoch 477, testing loss: 0.021905\n",
            "Epoch 478, testing loss: 0.021905\n",
            "Epoch 479, testing loss: 0.021905\n",
            "Epoch 480, testing loss: 0.021905\n",
            "Epoch 481, testing loss: 0.021905\n",
            "Epoch 482, testing loss: 0.021905\n",
            "Epoch 483, testing loss: 0.021905\n",
            "Epoch 484, testing loss: 0.021905\n",
            "Epoch 485, testing loss: 0.021905\n",
            "Epoch 486, testing loss: 0.021905\n",
            "Epoch 487, testing loss: 0.021905\n",
            "Epoch 488, testing loss: 0.021905\n",
            "Epoch 489, testing loss: 0.021905\n",
            "Epoch 490, testing loss: 0.021905\n",
            "Epoch 491, testing loss: 0.021905\n",
            "Epoch 492, testing loss: 0.021905\n",
            "Epoch 493, testing loss: 0.021905\n",
            "Epoch 494, testing loss: 0.021905\n",
            "Epoch 495, testing loss: 0.021905\n",
            "Epoch 496, testing loss: 0.021905\n",
            "Epoch 497, testing loss: 0.021905\n",
            "Epoch 498, testing loss: 0.021905\n",
            "Epoch 499, testing loss: 0.021905\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}